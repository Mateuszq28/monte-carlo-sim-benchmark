{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "porównanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, rc\n",
    "from scipy.stats import chisquare, kstest, norm, levene, ttest_ind, boxcox\n",
    "from scipy.stats import probplot, kurtosis, skew, spearmanr, wilcoxon, chi2_contingency\n",
    "from scipy.stats import mannwhitneyu, kruskal, pearsonr, ttest_rel, f_oneway\n",
    "import pylab\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from PIL import Image\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funkcje z compare.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_file(filename):\n",
    "    file = open(filename, 'r')\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def cube_list2array(d):\n",
    "    d['cube'] = np.asarray(d['cube'])\n",
    "\n",
    "\n",
    "def trim_cube(d, sh):\n",
    "    if d['cube'].shape != sh:\n",
    "        print(\"trimming\")\n",
    "        print(\"shape\", d['cube'].shape)\n",
    "        d['cube'] = d['cube'][:sh[0], :sh[1], :sh[2]]\n",
    "        print(\"shape\", d['cube'].shape)\n",
    "\n",
    "\n",
    "def load_from_filename_list(fn_list):\n",
    "    cube_list = []\n",
    "    for fn in fn_list:\n",
    "        cube = load_from_file(fn)\n",
    "        cube_list2array(cube)\n",
    "        trim_cube(cube, (180, 180, 240))\n",
    "        cube_list.append(cube)\n",
    "    return cube_list\n",
    "\n",
    "\n",
    "def cube_by_name(all_cubes, name):\n",
    "    for i in range(len(all_cubes)):\n",
    "        if all_cubes[i]['name'] == name:\n",
    "            return all_cubes[i].copy()\n",
    "    return None\n",
    "\n",
    "\n",
    "# próba kontrolna\n",
    "def add_random_cube(all_cubes):\n",
    "    if all_cubes[0]['name'] == 'random_cube':\n",
    "        print('ta seria ma już random_cube!')\n",
    "    else:\n",
    "        c = all_cubes[0]['cube'] # my params\n",
    "        sh0 = len(c)\n",
    "        sh1 = len(c[0])\n",
    "        sh2 = len(c[0][0])\n",
    "        cube_shape = [sh0, sh1, sh2]\n",
    "        cb = cube_shape\n",
    "\n",
    "        random_arr = np.random.rand(cb[0], cb[1], cb[2])\n",
    "        s = np.sum(random_arr)\n",
    "        n_random_photons = 100_000_000\n",
    "        random_arr *= n_random_photons\n",
    "        random_arr /= s\n",
    "\n",
    "        c2 = all_cubes[0]\n",
    "        random_cube = {\n",
    "            \"n_photons\": n_random_photons,\n",
    "            \"overflow\": 0,\n",
    "            \"mu_a\": 1.673,\n",
    "            \"name\": 'random_cube',\n",
    "            \"bins_per_1_cm\": c2['bins_per_1_cm'],\n",
    "            \"cube\": random_arr,\n",
    "            \"photon_weight\": 1.0,\n",
    "            \"normalized_already\": False,\n",
    "            \"file_path\": \"NA\",\n",
    "            \"file_dir_path\": \"NA\",\n",
    "            \"params_type\": \"org_my\"\n",
    "        }\n",
    "\n",
    "        print(random_arr.size)\n",
    "        print(random_arr.shape)\n",
    "        print(random_arr.sum())\n",
    "\n",
    "        all_cubes.insert(0, random_cube)\n",
    "\n",
    "\n",
    "def filter_outliers(arr, q_min=0.01, q_max=0.99):\n",
    "    arr = arr.copy()\n",
    "    if True:\n",
    "        qqmin = np.quantile(arr, q=q_min)\n",
    "        qqmax = np.quantile(arr, q=q_max)\n",
    "        main_ids_out = np.logical_or( (arr < qqmin),  (arr > qqmax) )\n",
    "        s = np.sum(main_ids_out)\n",
    "        val_replace = np.random.choice(arr.flatten(), s)\n",
    "        temp_arr = val_replace\n",
    "        while True:\n",
    "            ids_out = np.logical_or( (temp_arr < qqmin), (temp_arr > qqmax) )\n",
    "            s = np.sum(ids_out)\n",
    "            if s == 0:\n",
    "                arr[main_ids_out] = temp_arr\n",
    "                break\n",
    "            val_replace = np.random.choice(arr.flatten(), s)\n",
    "            temp_arr[ids_out] = val_replace\n",
    "    return arr\n",
    "\n",
    "\n",
    "def simple_hist_plot(arr, range, bins, title=\"\", density=False, norm=False):\n",
    "    # plt.hist(arr, range=range, bins=bins, density=density)\n",
    "    # # plt.yscale(\"log\") # - tutaj dopisać title i opisy osi\n",
    "    # plt.grid()\n",
    "    # plt.show() #  - zakomentować to\n",
    "    \n",
    "    hist, bin_edges = np.histogram(arr, range=range, bins=bins, density=density)\n",
    "    plt.bar(bin_edges[:-1], hist, width=(bin_edges[1] - bin_edges[0]) * 1.0) # , color='b'\n",
    "    # plt.plot(bin_edges[:-1], hist, c='r')\n",
    "    plt.title('Histogram'+title)\n",
    "    plt.xlabel('transport')\n",
    "    plt.ylabel('ilość próbek')\n",
    "    if norm == \"log\":\n",
    "        plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def simple_boxplot(arr, title=\"\"):\n",
    "    plt.boxplot(arr)\n",
    "    plt.title('Boxplot'+title)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def filter_zeros(arr, typ=None):\n",
    "    arr = arr.copy()\n",
    "    arr[arr == 0] = np.min(arr[arr > 0])\n",
    "    return arr\n",
    "\n",
    "\n",
    "def fun_tan(arr, typ=None):\n",
    "    return -np.tan(arr - np.pi / 2)\n",
    "\n",
    "\n",
    "def fun_tan_wrapper(arr, typ): # łamana\n",
    "    arr = arr.copy()\n",
    "    global max_my\n",
    "    global max_org\n",
    "    global brpoint_my\n",
    "    global brpoint_org\n",
    "\n",
    "    if typ is None:\n",
    "        raise ValueError('Potrzebny typ parametrów danych')\n",
    "    \n",
    "    if 'org' in typ:\n",
    "        maxi_all = max_org\n",
    "        brpoint = brpoint_org\n",
    "    elif 'my' in typ:\n",
    "        maxi_all = max_my\n",
    "        brpoint = brpoint_my\n",
    "    else:\n",
    "        raise ValueError('Potrzebny typ parametrów danych')\n",
    "\n",
    "    # dane od 0 do brpoint trzeba rozciągnąć na zakres 0 do pi/4\n",
    "    # tan(pi/4) = 1\n",
    "    arr = arr.copy()\n",
    "    new_arr = arr.copy()\n",
    "    new_arr[arr <= brpoint] = new_arr[arr <= brpoint] / brpoint * np.pi/4\n",
    "\n",
    "    # dane od brpoint do maxi_all trzeba rozciągnąć na zakres od pi/4 do pi/2\n",
    "    new_arr[arr > brpoint] = (new_arr[arr > brpoint] - brpoint) / (maxi_all-brpoint) * (np.pi/2 - np.pi/4) + np.pi/4\n",
    "\n",
    "    arr = new_arr\n",
    "    arr = fun_tan(arr)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "def move_mean(arr):\n",
    "    arr = arr.copy()\n",
    "    mea = np.mean(arr)\n",
    "    arr = arr - mea\n",
    "    return arr\n",
    "\n",
    "\n",
    "def move_std(arr):\n",
    "    arr = arr.copy()\n",
    "    mea = np.mean(arr)\n",
    "    st = np.std(arr)\n",
    "    arr = arr - mea\n",
    "    arr = arr / st\n",
    "    arr = arr + mea\n",
    "    return arr\n",
    "\n",
    "\n",
    "def move_mean_std(arr):\n",
    "    arr = arr.copy()\n",
    "    mea = np.mean(arr)\n",
    "    st = np.std(arr)\n",
    "    arr = arr - mea\n",
    "    arr = arr / st\n",
    "    return arr\n",
    "\n",
    "\n",
    "def dump_transformers():\n",
    "\n",
    "    model_dir = 'models'\n",
    "    files = os.listdir(model_dir)\n",
    "    files = [model_dir+'/'+fn for fn in files]\n",
    "    print(files)\n",
    "\n",
    "    global org_quantile_transformer\n",
    "    global org_power_transformer\n",
    "    global org_global_training_arr\n",
    "    global my_quantile_transformer\n",
    "    global my_power_transformer\n",
    "    global my_global_training_arr\n",
    "\n",
    "    # Ścieżki do zapisania modeli\n",
    "    org_quantile_transformer_filename = 'models/org_quantile_transformer.pkl'\n",
    "    org_power_transformer_filename = 'models/org_power_transformer.pkl'\n",
    "    org_global_training_arr_filename = 'models/org_global_training_arr.pkl'\n",
    "    my_quantile_transformer_filename = 'models/my_quantile_transformer.pkl'\n",
    "    my_power_transformer_filename = 'models/my_power_transformer.pkl'\n",
    "    my_global_training_arr_filename = 'models/my_global_training_arr.pkl'\n",
    "\n",
    "    # Zapisanie modeli za pomocą joblib\n",
    "    if org_quantile_transformer is not None and (org_quantile_transformer_filename not in files):\n",
    "        joblib.dump(org_quantile_transformer, org_quantile_transformer_filename)\n",
    "    if org_power_transformer is not None and (org_power_transformer_filename not in files):\n",
    "        joblib.dump(org_power_transformer, org_power_transformer_filename)\n",
    "    if org_global_training_arr is not None and (org_global_training_arr_filename not in files):\n",
    "        joblib.dump(org_global_training_arr, org_global_training_arr_filename)\n",
    "    if my_quantile_transformer is not None and (my_quantile_transformer_filename not in files):\n",
    "        joblib.dump(my_quantile_transformer, my_quantile_transformer_filename)\n",
    "    if my_power_transformer is not None and (my_power_transformer_filename not in files):\n",
    "        joblib.dump(my_power_transformer, my_power_transformer_filename)\n",
    "    if my_global_training_arr is not None and (my_global_training_arr_filename not in files):\n",
    "        joblib.dump(my_global_training_arr, my_global_training_arr_filename)\n",
    "\n",
    "\n",
    "def load_transformers():\n",
    "\n",
    "    model_dir = 'models'\n",
    "    files = os.listdir(model_dir)\n",
    "    files = [model_dir+'/'+fn for fn in files]\n",
    "    print(files)\n",
    "\n",
    "    loaded = 0\n",
    "\n",
    "    global org_quantile_transformer\n",
    "    global org_power_transformer\n",
    "    global org_global_training_arr\n",
    "    global my_quantile_transformer\n",
    "    global my_power_transformer\n",
    "    global my_global_training_arr\n",
    "\n",
    "    # Ścieżki do zapisania modeli\n",
    "    org_quantile_transformer_filename = 'models/org_quantile_transformer.pkl'\n",
    "    org_power_transformer_filename = 'models/org_power_transformer.pkl'\n",
    "    org_global_training_arr_filename = 'models/org_global_training_arr.pkl'\n",
    "    my_quantile_transformer_filename = 'models/my_quantile_transformer.pkl'\n",
    "    my_power_transformer_filename = 'models/my_power_transformer.pkl'\n",
    "    my_global_training_arr_filename = 'models/my_global_training_arr.pkl'\n",
    "\n",
    "    if org_quantile_transformer is None and (org_quantile_transformer_filename in files):\n",
    "        org_quantile_transformer = joblib.load(org_quantile_transformer_filename)\n",
    "        loaded += 1\n",
    "    if org_power_transformer is None and (org_power_transformer_filename in files):\n",
    "        org_power_transformer = joblib.load(org_power_transformer_filename)\n",
    "        loaded += 1\n",
    "    if org_global_training_arr is None and (org_global_training_arr_filename in files):\n",
    "        org_global_training_arr = joblib.load(org_global_training_arr_filename)\n",
    "        loaded += 1\n",
    "    if my_quantile_transformer is None and (my_quantile_transformer_filename in files):\n",
    "        my_quantile_transformer = joblib.load(my_quantile_transformer_filename)\n",
    "        loaded += 1\n",
    "    if my_power_transformer is None and (my_power_transformer_filename in files):\n",
    "        my_power_transformer = joblib.load(my_power_transformer_filename)\n",
    "        loaded += 1\n",
    "    if my_global_training_arr is None and (my_global_training_arr_filename in files):\n",
    "        my_global_training_arr = joblib.load(my_global_training_arr_filename)\n",
    "        loaded += 1\n",
    "\n",
    "    print(f'loaded num {loaded}')\n",
    "    if loaded < 6:\n",
    "        return False\n",
    "    elif loaded == 6:\n",
    "        return True\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "def train_data_transformers(init_arr = None):\n",
    "\n",
    "    if load_transformers():\n",
    "        return True\n",
    "\n",
    "    global quantile_transformer\n",
    "    global power_transformer\n",
    "    global global_training_arr\n",
    "\n",
    "    if quantile_transformer is None or power_transformer is None: # tutaj obsługowany\n",
    "        # jest wyjątek braku ustawienia wcześniej transformerów globalnych\n",
    "        if global_training_arr is None:\n",
    "            if init_arr is None:\n",
    "                raise Exception('Tablica treningowa nie została wcześniej ustawiona ani podana jako argument tej funkcji.')\n",
    "            else:\n",
    "                arr = init_arr.flatten()\n",
    "                arr = np.sort(arr)\n",
    "                arr = filter_outliers(arr)\n",
    "                global_training_arr = arr\n",
    "    else:\n",
    "        print('Wszystkie transformery zostały przetrenowane wcześniej.')\n",
    "\n",
    "    if quantile_transformer is None:\n",
    "        print('Trenowanie quantile transformer.')\n",
    "        rng = np.random.RandomState(304)\n",
    "        # n_quantiles is set to the training set size rather than the default value\n",
    "        # to avoid a warning being raised by this example\n",
    "        n_quantiles = global_training_arr.shape[0]\n",
    "        n_quantiles = 1_000_000\n",
    "        subsample = n_quantiles\n",
    "        qt = QuantileTransformer(\n",
    "        n_quantiles=n_quantiles, output_distribution=\"normal\", random_state=rng, subsample=subsample\n",
    "        )\n",
    "        quantile_transformer = qt.fit(global_training_arr.reshape(-1, 1))\n",
    "        print('Training quantile transformer done.')\n",
    "\n",
    "    if power_transformer is None:\n",
    "        print('Trenowanie power transformer.')\n",
    "        rng = np.random.RandomState(304)\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        power_transformer = pt.fit(global_training_arr.reshape(-1, 1))\n",
    "        print('Training power transformer done.')\n",
    "\n",
    "\n",
    "def setup_transformers(all_cubes):\n",
    "    # wczytaj lub wytrenuj i zapisz\n",
    "\n",
    "    global quantile_transformer\n",
    "    global power_transformer\n",
    "    global global_training_arr\n",
    "\n",
    "    global org_quantile_transformer\n",
    "    global org_power_transformer\n",
    "    global org_global_training_arr\n",
    "    global my_quantile_transformer\n",
    "    global my_power_transformer\n",
    "    global my_global_training_arr\n",
    "\n",
    "    quantile_transformer = None\n",
    "    power_transformer = None\n",
    "    global_training_arr = None\n",
    "\n",
    "    org_quantile_transformer = None\n",
    "    org_power_transformer = None\n",
    "    org_global_training_arr = None\n",
    "    my_quantile_transformer = None\n",
    "    my_power_transformer = None\n",
    "    my_global_training_arr = None\n",
    "\n",
    "    if load_transformers():\n",
    "        return True\n",
    "\n",
    "    train_arr = cube_by_name(all_cubes, 'mc456_my_100mln_cube')['cube']\n",
    "    train_data_transformers(init_arr = train_arr)\n",
    "    my_quantile_transformer = quantile_transformer\n",
    "    my_power_transformer = power_transformer\n",
    "    my_global_training_arr = global_training_arr\n",
    "    quantile_transformer = None\n",
    "    power_transformer = None\n",
    "    global_training_arr = None\n",
    "    print('my', my_quantile_transformer)\n",
    "    print('empty', quantile_transformer)\n",
    "\n",
    "    train_arr = cube_by_name(all_cubes, 'mc456_org_100mln_cube')['cube']\n",
    "    train_data_transformers(init_arr = train_arr)\n",
    "    org_quantile_transformer = quantile_transformer\n",
    "    org_power_transformer = power_transformer\n",
    "    org_global_training_arr = global_training_arr\n",
    "    quantile_transformer = None\n",
    "    power_transformer = None\n",
    "    global_training_arr = None\n",
    "\n",
    "    dump_transformers()\n",
    "\n",
    "\n",
    "def ttest_preprocess(arr: np.ndarray, typ, dec=1):\n",
    "\n",
    "    global org_power_transformer\n",
    "    global my_power_transformer\n",
    "    global org_quantile_transformer\n",
    "    global my_quantile_transformer\n",
    "\n",
    "    arr = arr.flatten()\n",
    "\n",
    "    if dec == -1:\n",
    "        return arr\n",
    "\n",
    "    arr = filter_outliers(arr)\n",
    "\n",
    "    if dec == 0:\n",
    "        return arr\n",
    "\n",
    "    if dec == 1:\n",
    "        arr = filter_zeros(arr)\n",
    "        arr = np.log(arr) # najlepszy boxcox lambda = 0, więc lepiej logarytm\n",
    "        return arr\n",
    "\n",
    "    if dec == 2:\n",
    "        arr = fun_tan_wrapper(arr, typ) # przeskalowany logarytm\n",
    "        return arr\n",
    "\n",
    "    if dec == 3:\n",
    "        if 'org' in typ:\n",
    "            transformer = org_power_transformer\n",
    "        elif 'my' in typ:\n",
    "            transformer = my_power_transformer\n",
    "        transformed_data = transformer.transform(arr.reshape(-1, 1)).reshape(-1)\n",
    "        arr = transformed_data.flatten()\n",
    "        return arr\n",
    "\n",
    "    if dec == 4:\n",
    "        if 'org' in typ:\n",
    "            transformer = org_quantile_transformer\n",
    "        elif 'my' in typ:\n",
    "            transformer = org_quantile_transformer\n",
    "        transformed_data = transformer.transform(arr.reshape(-1, 1)).reshape(-1)\n",
    "        arr = transformed_data.flatten()\n",
    "        return arr\n",
    "    \n",
    "    raise NotImplementedError('Nie ma takiego dec')\n",
    "\n",
    "\n",
    "def print_stats(cub_list, cub_arr_list=None, csv_out='print_stats_dump.csv'):\n",
    "    header_text = f\"{'Name' : >30}{'Sum' : >30}{'Avg' : >30}\"\n",
    "    header_text += f\"{'ks_normal' : >30}{'ks_normal pval' : >30}\"\n",
    "    header_text += f\"{'ks_normal log' : >30}{'ks_normal log pval' : >30}\"\n",
    "    header_text += f\"{'ks_normal tan' : >30}{'ks_normal tan pval' : >30}\"\n",
    "    header_text += f\"{'ks yeo-johnson' : >30}{'ks yeo-johnson pval' : >30}\"\n",
    "    header_text += f\"{'ks quantile' : >30}{'ks quantile pval' : >30}\"\n",
    "    header_text += f\"{'skew' : >30}{'kurtosis' : >30}\"\n",
    "    header_text += f\"{'q0' : >30}{'q0_25' : >30}{'q0_50' : >30}\"\n",
    "    header_text += f\"{'q0_75' : >30}{'q1' : >30}{'std' : >30}\"\n",
    "    header_text += f\"{'variance' : >30}{'Overflow' : >30}{'perc. in' : >30}\"\n",
    "    header_text += f\"{'n_photons' : >30}{'non_zero_vals' : >30}{'zero_vals' : >30}\"\n",
    "    print(header_text)\n",
    "    csv_list = ['Name', 'Sum', 'Avg']\n",
    "    csv_list += ['ks_normal', 'ks_normal pval']\n",
    "    csv_list += ['ks_normal log', 'ks_normal log pval']\n",
    "    csv_list += ['ks_normal tan', 'ks_normal tan pval']\n",
    "    csv_list += ['ks yeo-johnson', 'ks yeo-johnson pval']\n",
    "    csv_list += ['ks quantile', 'ks quantile pval']\n",
    "    csv_list += ['skew', 'kurtosis']\n",
    "    csv_list += ['q0', 'q0_25', 'q0_50']\n",
    "    csv_list += ['q0_75', 'q1', 'std']\n",
    "    csv_list += ['variance', 'Overflow', 'perc_in']\n",
    "    csv_list += ['n_photons', 'non_zero_vals', 'zero_vals']\n",
    "    csv_text = ','.join(csv_list) + '\\n'\n",
    "    csv_list = []\n",
    "    for i in range(len(cub_list)):\n",
    "        cub = cub_list[i]\n",
    "        if (cub_arr_list is None):\n",
    "            cub_arr = cub['cube'].copy()\n",
    "        else:\n",
    "            cub_arr = cub_arr_list[i].copy()\n",
    "        n_phot = cub['n_photons']\n",
    "        ovf = cub['overflow']\n",
    "        s = cub_arr.sum()\n",
    "        avg = np.average(cub_arr)\n",
    "        q0 = np.quantile(cub_arr, q=0.0)\n",
    "        q0_25 = np.quantile(cub_arr, q=0.25)\n",
    "        q0_50 = np.quantile(cub_arr, q=0.50)\n",
    "        q0_75 = np.quantile(cub_arr, q=0.75)\n",
    "        q1 = np.quantile(cub_arr, q=1.0)\n",
    "        std = np.std(cub_arr)\n",
    "        var = np.var(cub_arr)\n",
    "        perc = s / (s + ovf) * 100\n",
    "        non_zero_vals = cub_arr.size - np.isclose(cub_arr, 0).sum()\n",
    "        zero_vals = cub_arr.size - non_zero_vals\n",
    "        # testy na normalność\n",
    "        # test 1\n",
    "        data = ttest_preprocess(cub_arr, typ=cub['params_type'], dec=-1)\n",
    "        data = move_mean_std(data)\n",
    "        ks_normal_test = kstest(data, norm.cdf)\n",
    "        ks_normal_test_pval = ks_normal_test.pvalue\n",
    "        ks_normal_test = ks_normal_test.statistic\n",
    "        # test 2\n",
    "        data = ttest_preprocess(cub_arr, typ=cub['params_type'], dec=1)\n",
    "        data = move_mean_std(data)\n",
    "        ks_normal_test2 = kstest(data, norm.cdf)\n",
    "        ks_normal_test2_pval = ks_normal_test2.pvalue\n",
    "        ks_normal_test2 = ks_normal_test2.statistic\n",
    "        # test 3\n",
    "        data = ttest_preprocess(cub_arr, typ=cub['params_type'], dec=2)\n",
    "        data = move_mean_std(data)\n",
    "        ks_normal_test3 = kstest(data, norm.cdf)\n",
    "        ks_normal_test3_pval = ks_normal_test3.pvalue\n",
    "        ks_normal_test3 = ks_normal_test3.statistic\n",
    "        # test 4\n",
    "        data = ttest_preprocess(cub_arr, typ=cub['params_type'], dec=3)\n",
    "        data = move_mean_std(data)\n",
    "        ks_normal_test4 = kstest(data, norm.cdf)\n",
    "        ks_normal_test4_pval = ks_normal_test4.pvalue\n",
    "        ks_normal_test4 = ks_normal_test4.statistic\n",
    "        # test 5\n",
    "        data = ttest_preprocess(cub_arr, typ=cub['params_type'], dec=4)\n",
    "        data = move_mean_std(data)\n",
    "        ks_normal_test5 = kstest(data, norm.cdf)\n",
    "        ks_normal_test5_pval = ks_normal_test5.pvalue\n",
    "        ks_normal_test5 = ks_normal_test5.statistic\n",
    "        # skośność\n",
    "        ske = skew(cub_arr.flatten())\n",
    "        # kurtoza\n",
    "        kurt = kurtosis(cub_arr.flatten())\n",
    "        val_text = f\"{cub['name'] : >30}{s : >30,.2f}{avg : >30,.2f}\"\n",
    "        val_text += f\"{ks_normal_test : >30,.2f}{ks_normal_test_pval : >30,.2f}\"\n",
    "        val_text += f\"{ks_normal_test2 : >30,.2f}{ks_normal_test2_pval : >30,.2f}\"\n",
    "        val_text += f\"{ks_normal_test3 : >30,.2f}{ks_normal_test3_pval : >30,.2f}\"\n",
    "        val_text += f\"{ks_normal_test4 : >30,.2f}{ks_normal_test4_pval : >30,.2f}\"\n",
    "        val_text += f\"{ks_normal_test5 : >30,.2f}{ks_normal_test5_pval : >30,.2f}\"\n",
    "        val_text += f\"{ske : >30,.2f}{kurt : >30,.2f}\"\n",
    "        val_text += f\"{q0 : >30,.2f}{q0_25 : >30,.2f}{q0_50 : >30,.2f}\"\n",
    "        val_text += f\"{q0_75 : >30,.2f}{q1 : >30,.2f}{std : >30,.2f}\"\n",
    "        val_text += f\"{var : >30,.2f}{ovf : >30,.2f}{perc : >30,.2f}\"\n",
    "        val_text += f\"{n_phot : >30}{non_zero_vals : >30}{zero_vals : >30}\"\n",
    "        print(val_text)\n",
    "        csv_list = []\n",
    "        csv_list = [cub['name'], s, avg]\n",
    "        csv_list += [ks_normal_test, ks_normal_test_pval]\n",
    "        csv_list += [ks_normal_test2, ks_normal_test2_pval]\n",
    "        csv_list += [ks_normal_test3, ks_normal_test3_pval]\n",
    "        csv_list += [ks_normal_test4, ks_normal_test4_pval]\n",
    "        csv_list += [ks_normal_test5, ks_normal_test5_pval]\n",
    "        csv_list += [ske, kurt]\n",
    "        csv_list += [q0, q0_25, q0_50]\n",
    "        csv_list += [q0_75, q1, std]\n",
    "        csv_list += [var, ovf, perc]\n",
    "        csv_list += [n_phot, non_zero_vals, zero_vals]\n",
    "        csv_list = [str(n) for n in csv_list]\n",
    "        csv_text += ','.join(csv_list) + '\\n'\n",
    "        with open(csv_out, 'w') as f:\n",
    "            f.write(csv_text)\n",
    "\n",
    "\n",
    "def normalization(d):\n",
    "    if d['normalized_already']:\n",
    "        print('dane znormalizowano już wcześniej!')\n",
    "    else:\n",
    "        # bins_per_1_cm = 120\n",
    "        # mu_a = 0.37\n",
    "        # n_photons = 1_000_000\n",
    "        # val = val * 4.67\n",
    "        bin_size_in_cm = 1 / d['bins_per_1_cm']\n",
    "        bin_volume = (bin_size_in_cm)**3\n",
    "        multi = 1 / (d['n_photons'] * bin_volume * d['mu_a'])\n",
    "        d['cube'] = d['cube'] * multi\n",
    "        d['overflow'] = d['overflow'] * multi\n",
    "        d['photon_weight'] = d['photon_weight'] * multi\n",
    "        d['normalized_already'] = True\n",
    "\n",
    "\n",
    "def small_stat_table(all_cubes):\n",
    "    print(f\"{'max' :>10}{'name' :>30}{'params_type' :>10}{'max val' :>20}{'zero_perc' :>20}\")\n",
    "    i = 0\n",
    "    for c in all_cubes:\n",
    "        arr = c['cube']\n",
    "        arr = filter_outliers(arr)\n",
    "        maxi = np.max(arr)\n",
    "        zero_perc = np.sum(arr == 0) / arr.flatten().shape[0]\n",
    "        print(f\"{'max' :>10}{str(i)+' '+c['name'] :>30}{c['params_type'] :>10}{maxi :>20,.2f}{zero_perc :>20,.2f}\")\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def draw_raw_example(all_cubes, id=1, q_min=0.0, q_max=1.0):\n",
    "    c = all_cubes[id]\n",
    "    arr = c['cube']\n",
    "    arr = arr.flatten()\n",
    "    arr = filter_outliers(arr)\n",
    "    print(c['name'])\n",
    "    print('max_arr', np.max(arr))\n",
    "    rng = (np.quantile(arr, q=q_min), np.quantile(arr, q=q_max))\n",
    "    simple_hist_plot(arr, range=rng, bins=1000)\n",
    "    simple_hist_plot(arr, range=rng, bins=1000, norm='log')\n",
    "    simple_boxplot(arr)\n",
    "\n",
    "\n",
    "def my_qq_plot(arr, postsampling=None, bins=None, title=\"\", reg=True):\n",
    "    bins = arr.shape[0] if bins is None else bins\n",
    "    q_min = 1 / bins\n",
    "    q_max = 1 - q_min\n",
    "    cdf = np.linspace(q_min, q_max, bins, endpoint=True)\n",
    "    quantiles = np.sort(arr)\n",
    "    if bins != arr.shape[0]:\n",
    "        step = arr.shape[0] / bins\n",
    "        f_idx = np.arange(0, bins, 1) * step\n",
    "        i_idx = np.round(f_idx)\n",
    "        i_idx = i_idx.astype(int)\n",
    "        quantiles = quantiles[i_idx]\n",
    "    quantiles_normal = norm.ppf(cdf)\n",
    "    if postsampling is not None:\n",
    "        # step = quantiles.shape[0] / postsampling\n",
    "        step_weights = -(quantiles[1:] - quantiles[:-1]) / np.max(quantiles) + 1\n",
    "        step_weights = step_weights * postsampling\n",
    "        f_idx = np.cumsum(step_weights)\n",
    "        print('f_idx[-1]', f_idx[-1])\n",
    "        # f_idx = np.arange(0, postsampling, 1) * step_weights\n",
    "        i_idx = np.round(f_idx)\n",
    "        i_idx = i_idx.astype(int)\n",
    "        x = quantiles_normal[i_idx]\n",
    "        y = quantiles[i_idx]\n",
    "    else:\n",
    "        x = quantiles_normal\n",
    "        y = quantiles\n",
    "    if reg:\n",
    "        coef = np.polyfit(x,y,1)\n",
    "        poly1d_fn = np.poly1d(coef)\n",
    "        # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "        plt.plot(x,y, 'bx', x, poly1d_fn(x), '-r') #'--k'=black dashed line, 'yo' = yellow circle marker\n",
    "        x_min = np.min(x)\n",
    "        x_max = np.max(x)\n",
    "        plt.xlim(x_min, x_max)\n",
    "        y_min = np.min(( np.min(y), poly1d_fn(x_min) ))\n",
    "        y_max = np.max(( np.max(y), poly1d_fn(x_max) ))\n",
    "        plt.ylim(y_min, y_max)\n",
    "    else:\n",
    "        plt.scatter(x, y, marker='x', c='b')\n",
    "    plt.title('Wykres kwantyl-kwantyl')\n",
    "    plt.ylabel('Kwantyle próbek')\n",
    "    plt.xlabel('Teoretyczne kwantyle'+title)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return quantiles_normal, quantiles\n",
    "\n",
    "\n",
    "def qq_plot_wrapper(arr, title=\"\"):\n",
    "    arr = np.sort(arr)\n",
    "    # probplot returns: (osm, osr) tuple of ndarrays\n",
    "    # Tuple of theoretical quantiles (osm, or order statistic medians) and ordered responses (osr). osr is simply sorted input x\n",
    "    # print('probplot')\n",
    "    # probplot(arr, dist=\"norm\", plot=pylab)\n",
    "    # pylab.show()\n",
    "    print('statsmodels.api')\n",
    "    qqplot = sm.qqplot(arr, line='q') # do rozważenia - tu zmienić line='q' na line='r'\n",
    "    # Zmieniamy marker na 'x' dla punktów, ale zachowujemy linię regresji\n",
    "    for line in qqplot.gca().get_lines():\n",
    "        if line.get_linestyle() == 'None':  # Dotyczy punktów\n",
    "            line.set_marker('x')\n",
    "            line.set_color('b')  # Kolor niebieski dla punktów\n",
    "    pylab.title('Wykres kwantyl-kwantyl'+title)\n",
    "    pylab.ylabel('Kwantyle próbek')\n",
    "    pylab.xlabel('Teoretyczne kwantyle'+title)\n",
    "    pylab.grid()\n",
    "    pylab.show()\n",
    "    # print('my_qq_plot')\n",
    "    # my_qq_plot(arr, postsampling=None, bins=100, title=title, reg=True)\n",
    "\n",
    "\n",
    "def print_all_normal_charts(all_cubes, all_arrs=None, dec=4):\n",
    "    for i in range(len(all_cubes)):\n",
    "        if all_arrs is None:\n",
    "            arr = all_cubes[i]['cube'].copy()\n",
    "        else:\n",
    "            arr = all_arrs[i].copy()\n",
    "        bins = 1000\n",
    "        # przed\n",
    "        title = f\" cube id {i}\"+f' {dec}'\n",
    "        # print('przed')\n",
    "        # simple_hist_plot(arr, range=None, bins=bins, title=title)\n",
    "        # qq_plot_wrapper(arr, title=title)\n",
    "        # po\n",
    "        print('po')\n",
    "        trans_arr = ttest_preprocess(arr, typ=all_cubes[i]['params_type'], dec=dec)\n",
    "        simple_hist_plot(trans_arr, range=None, bins=bins, title=title)\n",
    "        simple_hist_plot(trans_arr, range=None, bins=bins, title=title, norm='log')\n",
    "        qq_plot_wrapper(trans_arr, title=title)\n",
    "\n",
    "\n",
    "def draw_fun(fun, start=-10, stop=10, title=\"\", typ=None):\n",
    "    x = np.linspace(start, stop, 1000, endpoint=True)\n",
    "    # x = np.arange(start, stop, np.pi/10)\n",
    "    y = fun(x, typ=typ)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_fun2(fun, start=-10, stop=10, title=\"\", typ=None, dec=1):\n",
    "    x = np.linspace(start, stop, 1000, endpoint=True)\n",
    "    # x = np.arange(start, stop, np.pi/10)\n",
    "    y = fun(x, typ=typ, dec=dec)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def find_max_hist_val(all_cubes):\n",
    "    # rng_hist1 = (0, max_hist_my) if type1 == 'my' else (0, max_hist_org)\n",
    "    # rng_hist2 = (0, max_hist_my) if type1 == 'my' else (0, max_hist_org)\n",
    "    global max_hist_my\n",
    "    global max_hist_org\n",
    "    max_hist_my = 0\n",
    "    max_hist_org = 0\n",
    "    for c in all_cubes:\n",
    "        typ = all_cubes['params_type']\n",
    "        filtered = filter_outliers(c['cube'])\n",
    "        this_max = np.max(filtered)\n",
    "        if typ == 'org':\n",
    "            if this_max > max_hist_org:\n",
    "                max_hist_org = this_max\n",
    "        if typ == 'my':\n",
    "            if this_max > max_hist_my:\n",
    "                max_hist_org = max_hist_my\n",
    "\n",
    "\n",
    "def cohens_d(c0, c1):\n",
    "    return (np.mean(c0) - np.mean(c1)) / (np.sqrt((np.std(c0) ** 2 + np.std(c1) ** 2) / 2))\n",
    "\n",
    "\n",
    "def spearmanr_wrapper(arr1, arr2):\n",
    "    non_zero_vals1 = np.count_nonzero(arr1)\n",
    "    non_zero_vals2 = np.count_nonzero(arr2)\n",
    "    if non_zero_vals1 > 0 and non_zero_vals2 > 0:\n",
    "        corr_spearman, pval_spearman = spearmanr(arr1.flatten(), arr2.flatten())\n",
    "    else:\n",
    "        corr_spearman = None\n",
    "        pval_spearman = None\n",
    "    return corr_spearman, pval_spearman\n",
    "\n",
    "\n",
    "def pearsonr_wrapper(arr1, arr2):\n",
    "    non_zero_vals1 = np.count_nonzero(arr1)\n",
    "    non_zero_vals2 = np.count_nonzero(arr2)\n",
    "    if non_zero_vals1 > 0 and non_zero_vals2 > 0:\n",
    "        corr_pearson, pval_pearson = pearsonr(arr1.flatten(), arr2.flatten())\n",
    "    else:\n",
    "        corr_pearson = None\n",
    "        pval_pearson = None\n",
    "    return corr_pearson, pval_pearson\n",
    "\n",
    "\n",
    "def chi2_contingency_flat_wrapper(arr1, arr2):\n",
    "    bins = 1000\n",
    "    arr1 = arr1.copy()\n",
    "    arr2 = arr2.copy()\n",
    "    # arr1 = filter_outliers(arr1)\n",
    "    # arr2 = filter_outliers(arr2)\n",
    "    min_both = np.min( (np.min(arr1), np.min(arr2)) )\n",
    "    max_both = np.max( (np.max(arr1), np.max(arr2)) )\n",
    "    range_hist = (min_both, max_both)\n",
    "    hist1_flat, _ = np.histogram(arr1.flatten(), bins=bins, density=False, range=range_hist)\n",
    "    hist2_flat, _ = np.histogram(arr2.flatten(), bins=bins, density=False, range=range_hist)\n",
    "    if np.sum(hist1_flat < 5) > 0:\n",
    "        return None, None\n",
    "    if np.sum(hist2_flat < 5) > 0:\n",
    "        return None, None\n",
    "    # tabela kontyngencji\n",
    "    # Każdy wiersz to częstość dla jednej zmiennej kategorialnej\n",
    "    # Każda kolumna to częstość dla drugiej zmiennej\n",
    "    data = np.array([hist1_flat, hist2_flat])\n",
    "    # Test Chi-kwadrat\n",
    "    stat, p_value, dof, expected = chi2_contingency(data)\n",
    "    return stat, p_value\n",
    "\n",
    "\n",
    "def chi2_contingency_3d_wrapper(arr1, arr2):\n",
    "    if len(arr1.shape) != 3:\n",
    "        return None, None\n",
    "    bins = 1000\n",
    "    arr1 = arr1.copy()\n",
    "    arr2 = arr2.copy()\n",
    "    # arr1 = filter_outliers(arr1)\n",
    "    # arr2 = filter_outliers(arr2)\n",
    "    min_both = np.min( (np.min(arr1), np.min(arr2)) )\n",
    "    max_both = np.max( (np.max(arr1), np.max(arr2)) )\n",
    "    range_hist = (min_both, max_both)\n",
    "    range_hist = (range_hist, range_hist, range_hist)\n",
    "    hist1_flat = np.histogramdd(arr1.reshape(-1, 3), bins=(10,10,10), range=range_hist, density=None, weights=None)[0]\n",
    "    hist2_flat = np.histogramdd(arr2.reshape(-1, 3), bins=(10,10,10), range=range_hist, density=None, weights=None)[0]\n",
    "    if np.sum(hist1_flat < 5) > 0:\n",
    "        return None, None\n",
    "    if np.sum(hist2_flat < 5) > 0:\n",
    "        return None, None\n",
    "    # tabela kontyngencji\n",
    "    # Każdy wiersz to częstość dla jednej zmiennej kategorialnej\n",
    "    # Każda kolumna to częstość dla drugiej zmiennej\n",
    "    data = np.array([hist1_flat.flatten(), hist2_flat.flatten()])\n",
    "    # Test Chi-kwadrat\n",
    "    stat, p_value, dof, expected = chi2_contingency(data)\n",
    "    return stat, p_value\n",
    "\n",
    "\n",
    "def chi2_contingency_2d_wrapper(arr1, arr2):\n",
    "    if len(arr1.shape) != 2:\n",
    "        return None, None\n",
    "    bins = 1000\n",
    "    arr1 = arr1.copy()\n",
    "    arr2 = arr2.copy()\n",
    "    # arr1 = filter_outliers(arr1)\n",
    "    # arr2 = filter_outliers(arr2)\n",
    "    min_both = np.min( (np.min(arr1), np.min(arr2)) )\n",
    "    max_both = np.max( (np.max(arr1), np.max(arr2)) )\n",
    "    range_hist = (min_both, max_both)\n",
    "    range_hist = (range_hist, range_hist)\n",
    "    hist1_flat = np.histogramdd(arr1.reshape(-1, 2), bins=(10,10), range=range_hist, density=None, weights=None)[0]\n",
    "    hist2_flat = np.histogramdd(arr2.reshape(-1, 2), bins=(10,10), range=range_hist, density=None, weights=None)[0]\n",
    "    if np.sum(hist1_flat < 5) > 0:\n",
    "        return None, None\n",
    "    if np.sum(hist2_flat < 5) > 0:\n",
    "        return None, None\n",
    "    # tabela kontyngencji\n",
    "    # Każdy wiersz to częstość dla jednej zmiennej kategorialnej\n",
    "    # Każda kolumna to częstość dla drugiej zmiennej\n",
    "    data = np.array([hist1_flat.flatten(), hist2_flat.flatten()])\n",
    "    # Test Chi-kwadrat\n",
    "    stat, p_value, dof, expected = chi2_contingency(data)\n",
    "    return stat, p_value\n",
    "\n",
    "\n",
    "def ks_wrapper(arr1, arr2):\n",
    "    ks = kstest(arr1, arr2)\n",
    "    return ks.statistic, ks.pvalue\n",
    "\n",
    "\n",
    "def levene_wrapper(arr1, arr2):\n",
    "    l = levene(arr1, arr2)\n",
    "    return l.statistic, l.pvalue\n",
    "\n",
    "\n",
    "def compare2cubes_universal(cub1, cub2, cub_arr1=None, cub_arr2=None):\n",
    "\n",
    "\n",
    "    # dla innych od rozkładu normalnego\n",
    "    # Korelacja Spearmana\n",
    "    # Test U Manna-Whitney’a (niezależne)\n",
    "    # Test Test T Wilcoxona (zależne)\n",
    "\n",
    "    # Test H Kruskala-Wallisa - dla serii po ilości fotonów\n",
    "\n",
    "    # Chi kwadrat (test niezależności)\n",
    "    #     porónanie 2 zmiennych\n",
    "    #     1 zmienna - rodzaj eksperymentu (skala nominalna)\n",
    "    #     2 zmienna - skala porządkowa - częśtość występowania wartości fluencji\n",
    "    #         czyli histogram (flatten lub 3D)\n",
    "    \n",
    "    # test Kolmogorova-Smirnova - porownanie rozkladow\n",
    "\n",
    "\n",
    "\n",
    "    # # dla rozkładu normalnego\n",
    "    # Korelacja Pearsona\n",
    "    # Test t Studenta dla prób niezależnych\t\n",
    "    # Test t Studenta dla prób zależnych\t\n",
    "    # Analiza wariancji (ANOVA) - dla serii po ilości fotonów\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # USTAWIENIA POCZĄTKOWE\n",
    "    # ============================================\n",
    "\n",
    "    name = cub1['name']\n",
    "\n",
    "    if cub_arr1 is None:\n",
    "        arr1 = cub1['cube']\n",
    "    else:\n",
    "        arr1 = cub_arr1\n",
    "    if cub_arr2 is None:\n",
    "        arr2 = cub2['cube']\n",
    "    else:\n",
    "        arr2 = cub_arr2\n",
    "    \n",
    "    if 'org' in cub1['params_type']:\n",
    "        type1 = 'org'\n",
    "    elif 'my' in cub1['params_type']:\n",
    "        type1 = 'my'\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    if 'org' in cub2['params_type']:\n",
    "        type2 = 'org'\n",
    "    elif 'my' in cub2['params_type']:\n",
    "        type2 = 'my'\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    arr1 = arr1.copy()\n",
    "    arr2 = arr2.copy()\n",
    "    arr1 = filter_outliers(arr1)\n",
    "    arr2 = filter_outliers(arr2)\n",
    "\n",
    "    \n",
    "\n",
    "    # TESTY NIEPARAMETRYCZNE - nie musi być rozkład normalny\n",
    "    # ============================================\n",
    "    corr_spearman, pval_spearman = spearmanr_wrapper(arr1.flatten(), arr2.flatten())\n",
    "    t_stat, t_pval = wilcoxon(arr1.flatten(), arr2.flatten())\n",
    "    u_stat, u_pval = mannwhitneyu(arr1.flatten(), arr2.flatten())\n",
    "    chi2_flat_stat, chi2_flat_pval = chi2_contingency_flat_wrapper(arr1.flatten(), arr2.flatten())\n",
    "    chi2_2d_stat, chi2_2d_pval = chi2_contingency_2d_wrapper(arr1, arr2)\n",
    "    chi2_3d_stat, chi2_3d_pval = chi2_contingency_3d_wrapper(arr1, arr2)\n",
    "\n",
    "\n",
    " \n",
    "    # UNIWERSALNE\n",
    "    # ============================================\n",
    "\n",
    "    # ------- RMSE -------\n",
    "    mse = np.square(np.subtract(arr1, arr2)).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # ------- is normal? -------\n",
    "    ks_stat1, ks_pval1 = ks_wrapper(arr1.flatten(), norm.cdf)\n",
    "    ks_stat2, ks_pval2 = ks_wrapper(arr2.flatten(), norm.cdf)\n",
    "\n",
    "    # ------- czy ten sam rozkład? -------\n",
    "    ks_test_eq, ks_pval_eq = ks_wrapper(arr1.flatten(), arr2.flatten())\n",
    "\n",
    "    # ------- homogeniczne wariancje -------\n",
    "    levene_stat, levene_pval = levene_wrapper(arr1.flatten(), arr2.flatten())\n",
    "\n",
    "\n",
    "\n",
    "    # TESTY PARAMETRYCZNE - nie musi być rozkład normalny\n",
    "    # ============================================\n",
    "    corr_pearson, pval_pearson = pearsonr_wrapper(arr1.flatten(), arr2.flatten())\n",
    "    ttind_stat, ttind_pval = ttest_ind(arr1.flatten(), arr2.flatten())\n",
    "    ttrel_stat, ttrel_pval = ttest_rel(arr1.flatten(), arr2.flatten())\n",
    "\n",
    "\n",
    "\n",
    "    # ------- print -------\n",
    "    print_text = f\"{name : >30}\"\n",
    "    print_text += f\"{corr_spearman : >30,.2f}\" if corr_spearman != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{pval_spearman : >30,.2f}\" if pval_spearman != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{t_stat : >30,.2f}\" if t_stat != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{t_pval : >30,.2f}\" if t_pval != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{u_stat : >30,.2f}\" if u_stat != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{u_pval : >30,.2f}\" if u_pval != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{chi2_flat_stat : >30,.2f}\" if chi2_flat_stat != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{chi2_flat_pval : >30,.2f}\" if chi2_flat_pval != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{chi2_2d_stat : >30,.2f}\" if chi2_2d_stat != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{chi2_2d_pval : >30,.2f}\" if chi2_2d_pval != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{chi2_3d_stat : >30,.2f}\" if chi2_3d_stat != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{chi2_3d_pval : >30,.2f}\" if chi2_3d_pval != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{mse : >30,.2f}{rmse : >30,.2}\"\n",
    "    print_text += f\"{ks_stat1 : >30,.2f}\" if ks_stat1 != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{ks_pval1 : >30,.2f}\" if ks_pval1 != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{ks_stat2 : >30,.2f}\" if ks_stat2 != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{ks_pval2 : >30,.2f}\" if ks_pval2 != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{ks_test_eq : >30,.2f}\" if ks_test_eq != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{ks_pval_eq : >30,.2f}\" if ks_pval_eq != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{levene_stat : >30,.2f}\" if levene_stat != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{levene_pval : >30,.2f}\" if levene_pval != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{corr_pearson : >30,.2f}\" if corr_pearson != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{pval_pearson : >30,.2f}\" if pval_pearson != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{ttind_stat : >30,.2f}\" if ttind_stat != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{ttind_pval : >30,.2f}\" if ttind_pval != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{ttrel_stat : >30,.2f}\" if ttrel_stat != None else f\"{\"-\" : >30}\"\n",
    "    print_text += f\"{ttrel_pval : >30,.2f}\" if ttrel_pval != None else f\"{\"-\" : >30}\"\n",
    "    print(print_text)\n",
    "    csv_list = [name, corr_spearman, pval_spearman, t_stat, t_pval, u_stat, u_pval, chi2_flat_stat]\n",
    "    csv_list += [chi2_flat_pval, chi2_2d_stat, chi2_2d_pval, chi2_3d_stat, chi2_3d_pval, mse, rmse]\n",
    "    csv_list += [ks_stat1, ks_pval1, ks_stat2, ks_pval2, ks_test_eq, ks_pval_eq, levene_stat]\n",
    "    csv_list += [levene_pval, corr_pearson, pval_pearson, ttind_stat, ttind_pval, ttrel_stat, ttrel_pval]\n",
    "    csv_list = [str(val) for val in csv_list]\n",
    "    line = ','.join(csv_list) + '\\n'\n",
    "    return line\n",
    "\n",
    "\n",
    "def compare_all_cubes_universal(other_cub_list, benchmark_cub, other_arr_list=None, benchmark_arr=None, csv_out='compare_all_cubes_universal_dump.csv'):\n",
    "    print(f\"Comparison * to benchmark ({benchmark_cub['name']})\")\n",
    "    print_text = f\"{'name' : >30}\"\n",
    "    print_text += f\"{'corr spearman' : >30}\"\n",
    "    print_text += f\"{'pval spearman' : >30}\"\n",
    "    print_text += f\"{'t_stat' : >30}\"\n",
    "    print_text += f\"{'t_pval' : >30}\"\n",
    "    print_text += f\"{'u_stat' : >30}\"\n",
    "    print_text += f\"{'u_pval' : >30}\"\n",
    "    print_text += f\"{'chi2_flat_stat' : >30}\"\n",
    "    print_text += f\"{'chi2_flat_pval' : >30}\"\n",
    "    print_text += f\"{'chi2_2d_stat' : >30}\"\n",
    "    print_text += f\"{'chi2_2d_pval' : >30}\"\n",
    "    print_text += f\"{'chi2_3d_stat' : >30}\"\n",
    "    print_text += f\"{'chi2_3d_pval' : >30}\"\n",
    "    print_text += f\"{'MSE' : >30}{'RMSE' : >30}\"\n",
    "    print_text += f\"{'ks_stat1' : >30}\"\n",
    "    print_text += f\"{'ks_pval1' : >30}\"\n",
    "    print_text += f\"{'ks_stat2' : >30}\"\n",
    "    print_text += f\"{'ks_pval2' : >30}\"\n",
    "    print_text += f\"{'ks_test_eq' : >30}\"\n",
    "    print_text += f\"{'ks_pval_eq' : >30}\"\n",
    "    print_text += f\"{'levene_stat' : >30}\"\n",
    "    print_text += f\"{'levene_pval' : >30}\"\n",
    "    print_text += f\"{'corr_pearson' : >30}\"\n",
    "    print_text += f\"{'pval_pearson' : >30}\"\n",
    "    print_text += f\"{'ttind_stat' : >30}\"\n",
    "    print_text += f\"{'ttind_pval' : >30}\"\n",
    "    print_text += f\"{'ttrel_stat' : >30}\"\n",
    "    print_text += f\"{'ttrel_pval' : >30}\"\n",
    "    print(print_text)\n",
    "    csv_list = ['name', 'corr spearman', 'pval spearman', 't_stat', 't_pval', 'u_stat', 'u_pval']\n",
    "    csv_list += ['chi2_flat_stat', 'chi2_flat_pval', 'chi2_2d_stat', 'chi2_2d_pval', 'chi2_3d_stat']\n",
    "    csv_list += ['chi2_3d_pval', 'MSE', 'RMSE', 'ks_stat1', 'ks_pval1', 'ks_stat2', 'ks_pval2', 'ks_test_eq']\n",
    "    csv_list += ['ks_pval_eq', 'levene_stat', 'levene_pval', 'corr_pearson', 'pval_pearson']\n",
    "    csv_list += ['ttind_stat', 'ttind_pval', 'ttrel_stat', 'ttrel_pval']\n",
    "    csv_text = ','.join(csv_list) + '\\n'\n",
    "    csv_list = []\n",
    "    for i in range(len(other_cub_list)):\n",
    "        if other_arr_list is not None:\n",
    "            other_arr = other_arr_list[i]\n",
    "        else:\n",
    "            other_arr = None\n",
    "        line = compare2cubes_universal(other_cub_list[i], benchmark_cub, cub_arr1=other_arr, cub_arr2=benchmark_arr)\n",
    "        csv_text += line\n",
    "    with open(csv_out, 'w') as f:\n",
    "        f.write(csv_text)\n",
    "\n",
    "\n",
    "def corr_table_universal(all_cubes, all_arr=None):\n",
    "    if all_arr is None:\n",
    "        flat_arrs = [c['cube'].flatten() for c in all_cubes]\n",
    "    else:\n",
    "        flat_arrs = [a.flatten() for a in all_arr]\n",
    "    print('\\ncorrtable of flatten cubes')\n",
    "    corrtable = np.corrcoef(flat_arrs)\n",
    "    print(corrtable)\n",
    "\n",
    "    print()\n",
    "    names = [c['name'] for c in all_cubes]\n",
    "    print(names)\n",
    "\n",
    "\n",
    "def kruskal_wrapper(all_cubes):\n",
    "    # Test Kruskala-Wallisa\n",
    "    arrs = [c['cube'].flatten() for c in all_cubes]\n",
    "    # arrs = np.array(arrs)\n",
    "    # print(arrs.shape)\n",
    "    stat, p_value = kruskal(*arrs)\n",
    "\n",
    "    print('test nieparametryczny')\n",
    "    print(f\"Statystyka testu Kruskala-Wallisa: {stat}\")\n",
    "    print(f\"Wartość p: {p_value}\")\n",
    "\n",
    "\n",
    "def anova_wrapper(all_cubes):\n",
    "    # Analiza wariancji (ANOVA)\n",
    "    arrs = [c['cube'] for c in all_cubes]\n",
    "    stat, p_value = f_oneway(*arrs)\n",
    "\n",
    "    print('test parametryczny')\n",
    "    print('Analiza wariancji')\n",
    "    print(f\"Statystyka ANOVA F: {stat}\")\n",
    "    print(f\"Wartość p: {p_value}\")\n",
    "\n",
    "\n",
    "def make_frames(cub_list, arr_list=None):\n",
    "    for i in range(len(cub_list)):\n",
    "        if arr_list is None:\n",
    "            arr = cub_list[i]['cube']\n",
    "        else:\n",
    "            arr = arr_list[i]\n",
    "\n",
    "        sh = arr.shape\n",
    "        f = [None for _ in range(20)]\n",
    "\n",
    "        # --- x_high ---\n",
    "        # z tej strony wbijamy nóż cięcia slice\n",
    "\n",
    "        # 3 vertical slices\n",
    "        f[0] = arr[:,0,:]\n",
    "        f[1] = arr[:,sh[1]//2,:]\n",
    "        f[2] = arr[:,-1,:]\n",
    "        \n",
    "        # 3 horizontal slices\n",
    "        f[3] = arr[:,:,-1]\n",
    "        f[4] = arr[:,:,sh[0]//2]\n",
    "        f[5] = arr[:,:,0]\n",
    "\n",
    "        # sums have to be normalized -> division\n",
    "        # avg y\n",
    "        f[6] = arr.sum(axis=1) / sh[1]\n",
    "        # avg z\n",
    "        f[7] = arr.sum(axis=2) / sh[2]\n",
    "\n",
    "        # --- z_high ---\n",
    "        # z tej strony wbijamy nóż cięcia slice\n",
    "\n",
    "        # 3 vertical slices\n",
    "        sh = arr.shape\n",
    "        f[8] = f[0]\n",
    "        f[9] = f[1]\n",
    "        f[10] = f[2]\n",
    "        \n",
    "        # 3 horizontal slices\n",
    "        f[11] = arr[0,:,:]\n",
    "        f[12] = arr[sh[0]//2,:,:]\n",
    "        f[13] = arr[-1,:,:]\n",
    "\n",
    "        # avg y\n",
    "        f[14] = f[6]\n",
    "        # avg x\n",
    "        f[15] = arr.sum(axis=0) / sh[0]\n",
    "\n",
    "        # --- 1D - sum along 2 axes\n",
    "\n",
    "        # avg xy\n",
    "        f[16] = arr.sum(axis=(0,1)) / sh[0] / sh[1]\n",
    "        # avg xz\n",
    "        f[17] = arr.sum(axis=(0,2)) / sh[0] / sh[2]\n",
    "\n",
    "        # avg yz\n",
    "        f[18] = arr.sum(axis=(1,2)) / sh[1] / sh[2]\n",
    "        # avg xz\n",
    "        f[19] = f[17]\n",
    "\n",
    "        # --- save frames ---\n",
    "\n",
    "        bpc = cub_list[i]['bins_per_1_cm']\n",
    "        cub_list[i]['frames'] = f\n",
    "        cub_list[i]['frame_names'] = [\n",
    "            f\"y_low, slice, const y={0: .2f}\",\n",
    "            f\"y_low, slice, const y={sh[1]/2/bpc: .2f}\",\n",
    "            f\"y_low, slice, const y={sh[1]/bpc: .2f}\",\n",
    "\n",
    "            f\"z_high, slice, const z={sh[2]/bpc: .2f}\",\n",
    "            f\"z_high, slice, const z={sh[2]/2/bpc: .2f}\",\n",
    "            f\"z_high, slice, const z={0.0: .2f}\",\n",
    "\n",
    "            f\"y_low\\nśrednia wzdłuż osi y\",\n",
    "            f\"z_high\\nśrednia wzdłuż osi z\",\n",
    "\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "\n",
    "            f\"x_high, slice, const x={0.0: .2f}\",\n",
    "            f\"x_high, slice, const x={sh[0]/2/bpc: .2f}\",\n",
    "            f\"x_high, slice, const x={sh[0]/bpc: .2f}\",\n",
    "\n",
    "            \"\",\n",
    "            f\"x_high\\nśrednia wzdłuż osi x\",\n",
    "\n",
    "\n",
    "            f\"średnia wzdłuż osi x oraz y\",\n",
    "            f\"średnia wzdłuż osi x oraz z\",\n",
    "            f\"średnia wzdłuż osi y oraz z\",\n",
    "            \"\",\n",
    "        ]\n",
    "\n",
    "        cub_list[i]['frame_shortnames'] = [\n",
    "            \"y_low slice\",\n",
    "            \"y_low slice\",\n",
    "            \"y_low slice\",\n",
    "\n",
    "            \"z_high slice\",\n",
    "            \"z_high slice\",\n",
    "            \"z_high slice\",\n",
    "\n",
    "            \"avg_y y_low\",\n",
    "            \"avg_z z_high\",\n",
    "\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "\n",
    "            \"x_high slice\",\n",
    "            \"x_high slice\",\n",
    "            \"x_high slice\",\n",
    "\n",
    "            \"\",\n",
    "            \"avg_x x_high\",\n",
    "\n",
    "\n",
    "            f\"avg_xy\",\n",
    "            f\"avg_xz\",\n",
    "            f\"avg_yz\",\n",
    "            \"\",\n",
    "        ]\n",
    "\n",
    "\n",
    "# ------------------------------- HEAT MAPS -------------------------------\n",
    "\n",
    "# WARNING! if arr it's a sum along axis, remmember to divide by len(of this axis)\n",
    "# do it before giving it to heatmap2d function\n",
    "def heatmap2d(arr: np.ndarray, bins_per_cm, cube=None, title=None, norm=\"log\"):\n",
    "\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    # skip first tick on x axis\n",
    "    # ax = plt.gca()\n",
    "    # xticks = ax.xaxis.get_major_ticks()\n",
    "    # xticks[0].label1.set_visible(False)\n",
    "    \n",
    "    title_out, xlab, ylab, extent, plot_arr = handle_plot_data(arr, title, bins_per_cm, cube=cube)\n",
    "\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.title(title_out, pad=10, linespacing=1.5)\n",
    "\n",
    "    # wyświetlenie tablicy\n",
    "    plt.imshow(plot_arr, cmap='viridis', norm=norm, interpolation=\"none\", extent=extent) # musi być przed color bar\n",
    "\n",
    "    # kolorowy pasek ze skalą\n",
    "    # cb = plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    cb = plt.colorbar(pad=0.010)\n",
    "    cb.set_label(r'Fl [$ 1/cm^2 $]')\n",
    "\n",
    "    # manager = plt.get_current_fig_manager()\n",
    "    # manager.window.showMaximized()\n",
    "    # manager.canvas.toolbar.save_figure()\n",
    "\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def handle_plot_data(arr, title, bins_per_cm, cube=None):\n",
    "    \"\"\"\n",
    "    Funkcja pomocnicza do obsługi różnych przypadków osi i przekształceń\n",
    "    \"\"\"\n",
    "    xlab = \"\"\n",
    "    ylab = \"\"\n",
    "    title_out = \"Współczynnik fluencji względnej Fl\"\n",
    "    title_out += \"\\n\" + title if title is not None else \"\"\n",
    "    title_out += \"\\n\" + cube['name'] if cube is not None else \"\"\n",
    "    if \"x_high\" in title_out:\n",
    "        xlab = \"y [cm]\"\n",
    "        ylab = \"z [cm]\"\n",
    "        plot_arr = np.flip(arr.transpose(), axis=(0))\n",
    "        extent=[0, arr.shape[0]/bins_per_cm, 0, arr.shape[1]/bins_per_cm] # podziałka legendy\n",
    "    elif \"x_low\" in title_out:\n",
    "        xlab = \"y [cm]\"\n",
    "        ylab = \"z [cm]\"\n",
    "    elif \"y_high\" in title_out:\n",
    "        xlab = \"x [cm]\"\n",
    "        ylab = \"z [cm]\"\n",
    "    elif \"y_low\" in title_out:\n",
    "        xlab = \"x [cm]\"\n",
    "        ylab = \"z [cm]\"\n",
    "        # obracanie tablicy\n",
    "        plot_arr = np.flip(arr.transpose(), axis=(0))\n",
    "        extent=[0, arr.shape[0]/bins_per_cm, 0, arr.shape[1]/bins_per_cm] # podziałka legendy\n",
    "    elif \"z_high\" in title_out:\n",
    "        xlab = \"y [cm]\"\n",
    "        ylab = \"x [cm]\"\n",
    "        # obracanie tablicy\n",
    "        # plot_arr = np.flip(arr.transpose(), axis=(0))\n",
    "        plot_arr = arr\n",
    "        extent=[0, arr.shape[0]/bins_per_cm, arr.shape[1]/bins_per_cm, 0] # podziałka legendy\n",
    "        plt.gca().xaxis.set_label_position('top')  # Move xlabel to the top\n",
    "        plt.gca().xaxis.tick_top()  # Move the ticks to the top\n",
    "    elif \"z_low\" in title_out:\n",
    "        xlab = \"y [cm]\"\n",
    "        ylab = \"x [cm]\"\n",
    "    else:\n",
    "        plot_arr = arr\n",
    "    return title_out, xlab, ylab, extent, plot_arr\n",
    "\n",
    "\n",
    "def heatmap2d_grid_shared_colorbar(arr_list: list, bins_per_cm, grid_shape: tuple, cube_list=None, main_title=None, norm=\"log\"):\n",
    "    \"\"\"\n",
    "    Wyświetla tablice w układzie siatki (grid) z wspólną skalą kolorów i jednym colorbar.\n",
    "    \n",
    "    Parametry:\n",
    "    - arr_list: lista tablic numpy, które mają być wyświetlone,\n",
    "    - bins_per_cm: wartość określająca ilość binów na centymetr,\n",
    "    - grid_shape: krotka (rows, cols) określająca ilość wierszy i kolumn w siatce,\n",
    "    - main_title: wspólny tytuł dla wszystkich wykresów, opcjonalny,\n",
    "    - norm: sposób normalizacji, domyślnie \"log\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Obliczanie wspólnych minimalnych i maksymalnych wartości dla wszystkich tablic\n",
    "    global_min = np.min([np.min(arr) for arr in arr_list])\n",
    "    global_max = np.max([np.max(arr) for arr in arr_list])\n",
    "\n",
    "    # Wybór normy\n",
    "    if norm == \"log\":\n",
    "        norm = LogNorm(vmin=global_min, vmax=global_max)\n",
    "    else:\n",
    "        norm = Normalize(vmin=global_min, vmax=global_max)\n",
    "\n",
    "    # Inicjalizacja wykresu z określoną siatką\n",
    "    rows, cols = grid_shape\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "\n",
    "    # Zamiana jednowymiarowej tablicy axes na dwuwymiarową, jeśli siatka jest większa niż 1x1\n",
    "    if rows * cols > 1:\n",
    "        axes = axes.ravel()\n",
    "\n",
    "    # Przejście przez wszystkie tablice i rysowanie ich w odpowiednich podwykresach\n",
    "    for i, arr in enumerate(arr_list):\n",
    "        ax = axes[i] if rows * cols > 1 else axes\n",
    "\n",
    "        # Obsługa osi i przekształceń\n",
    "        title_out, xlab, ylab, extent, plot_arr = handle_plot_data(arr, main_title, bins_per_cm)\n",
    "        small_title = cube_list[i]['name']\n",
    "        ax.set_title(small_title, pad=10)\n",
    "\n",
    "        # Rysowanie wykresu\n",
    "        ax.set_xlabel(xlab, fontsize=14)\n",
    "        ax.set_ylabel(ylab, fontsize=14)\n",
    "\n",
    "        # Rysowanie mapy ciepła z wspólną normą dla wszystkich wykresów\n",
    "        im = ax.imshow(plot_arr, cmap='viridis', norm=norm, interpolation=\"none\", extent=extent)\n",
    "\n",
    "        # Zwiększenie rozmiaru czcionek na osiach\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    # Dodanie wspólnego colorbara po prawej stronie\n",
    "    fig.subplots_adjust(left=0.1, right=0.85, wspace=0.05, hspace=0.35)  # Minimalny odstęp między wykresami\n",
    "    cbar_ax = fig.add_axes([0.88, 0.1, 0.03, 0.8])  # Zmniejszenie odstępu między wykresami a colorbar\n",
    "    cb = fig.colorbar(im, cax=cbar_ax)\n",
    "    cb.set_label(r'Fl [$ 1/cm^2 $]', fontsize=14, linespacing=1.5)  # Etykieta paska kolorów\n",
    "    cb.ax.tick_params(labelsize=14)  # Zwiększenie czcionki skali na colorbar\n",
    "\n",
    "    # Ustawienie wspólnego tytułu nad wszystkimi wykresami, wyżej, aby nie nachodził na wykresy\n",
    "    if main_title is not None:\n",
    "        fig.suptitle(title_out, fontsize=20, y=1.25)  # Przesunięcie tytułu wyżej\n",
    "\n",
    "    plt.savefig('chart4_img/chart4.png', bbox_inches='tight')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------- PRINT ALL HEATMAPS -------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def print_all_frames(all_cubes, frame_ids=None):\n",
    "    frame_ids = list(range(0,8))+list(range(11,14))+[15] if frame_ids is None else frame_ids\n",
    "    for c in range(len(all_cubes)):\n",
    "        for i in frame_ids:\n",
    "            bins_per_cm = all_cubes[0]['bins_per_1_cm']\n",
    "            title = all_cubes[0]['frame_names'][i]\n",
    "            arr1 = all_cubes[c]['frames'][i]\n",
    "\n",
    "            # check axis\n",
    "            arr1 = arr1.copy()\n",
    "            arr1[0:20, 0:20] = 20 # niskie wartości, początek (0,0)\n",
    "            arr1[-20:-1, -20:-1] = 60 # wysokie wartości, koniec (1.5, 2.0)\n",
    "\n",
    "            heatmap2d(arr1, bins_per_cm, title=title)\n",
    "\n",
    "\n",
    "def print_all_frames_beside_benchmark(all_cubes, benchmark_cube, frame_ids=None):\n",
    "    frame_ids = list(range(0,8))+list(range(11,14))+[15] if frame_ids is None else frame_ids\n",
    "    for c in range(len(all_cubes)):\n",
    "        for i in frame_ids:\n",
    "            bins_per_cm = all_cubes[0]['bins_per_1_cm']\n",
    "            title = all_cubes[0]['frame_names'][i]\n",
    "            arr1 = all_cubes[c]['frames'][i]\n",
    "\n",
    "            # check axis\n",
    "            arr1 = arr1.copy()\n",
    "            arr1[0:20, 0:20] = 20 # niskie wartości, początek (0,0)\n",
    "            arr1[-20:-1, -20:-1] = 60 # wysokie wartości, koniec (1.5, 2.0)\n",
    "\n",
    "            cube_list = [arr1, benchmark_cube['frames'][i]]\n",
    "            heatmap2d_grid_shared_colorbar(cube_list, bins_per_cm, grid_shape=(1,2), main_title=title)\n",
    "\n",
    "\n",
    "def detect_and_split_image_with_vertical_trim(image_path, output_path, gap_threshold=20, pixel_threshold=220, new_gap=20):\n",
    "    \"\"\"\n",
    "    Funkcja wczytuje obraz, wykrywa poziome przerwy szersze niż gap_threshold - wycina je, a w dolnej części, pod tym wyciętym pasem\n",
    "    obrazu wykrywa białe kolumny pionowe powyżej pixel_threshold i zmniejsza je do 10% ich pierwotnej szerokości.\n",
    "    Następnie łączy górną i dolną część w jeden obraz.\n",
    "\n",
    "    Parametry:\n",
    "    - image_path: ścieżka do pliku wejściowego .png\n",
    "    - output_path: ścieżka do zapisu wynikowego obrazu\n",
    "    - gap_threshold: minimalna szerokość przerwy, aby została uznana za \"przerwę\"\n",
    "    - pixel_threshold: próg jasności dla wykrywania białych pikseli (im mniejsza wartość, tym większa czułość)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Wczytanie obrazu\n",
    "    img = Image.open(image_path)\n",
    "    img_color = img.copy()  # Zachowanie oryginalnego obrazu w kolorze\n",
    "    img_gray = img.convert(\"L\")  # Konwersja do skali szarości tylko dla operacji detekcji\n",
    "    img_array = np.array(img_gray)  # Konwersja do tablicy numpy\n",
    "\n",
    "    # Szukamy poziomych przerw (obszarów, gdzie piksele są \"białe\" lub bardzo jasne na całej szerokości)\n",
    "    horizontal_sum = np.sum(img_array > pixel_threshold, axis=1)  # Liczymy piksele jaśniejsze niż pixel_threshold\n",
    "    img_width = img_array.shape[1]\n",
    "\n",
    "    # Zwiększamy dokładność wykrywania przerw\n",
    "    gaps = np.where(horizontal_sum >= img_width * 1.0)[0]  # Wiersze, gdzie cała szerokość wiersza jest biała\n",
    "\n",
    "    # Szukamy linii, gdzie przerwa jest większa niż gap_threshold\n",
    "    large_gaps = []\n",
    "    current_gap = []\n",
    "    for i in range(1, len(gaps)):\n",
    "        if gaps[i] - gaps[i-1] > 1:  # Kiedy przerwa między kolejnymi białymi liniami jest większa niż 1\n",
    "            if len(current_gap) > gap_threshold:  # Jeśli przerwa ma odpowiednią szerokość\n",
    "                large_gaps.append(current_gap)\n",
    "            current_gap = []  # Resetujemy przerwę\n",
    "        current_gap.append(gaps[i])\n",
    "\n",
    "    if len(current_gap) > gap_threshold:\n",
    "        large_gaps.append(current_gap)\n",
    "\n",
    "    if not large_gaps:\n",
    "        raise ValueError(\"Nie znaleziono żadnej przerwy większej niż threshold.\")\n",
    "\n",
    "    # Wybieramy pierwszą dużą przerwę\n",
    "    first_large_gap = large_gaps[0]\n",
    "    gap_start = first_large_gap[0]\n",
    "    gap_end = first_large_gap[-1]\n",
    "\n",
    "    # Dzielimy obraz na górną i dolną część\n",
    "    top_part = img_color.crop((0, 0, img.width, gap_start))\n",
    "    bottom_part = img_color.crop((0, gap_end, img.width, img.height))\n",
    "    bottom_array = np.array(bottom_part.convert(\"L\"))  # Skala szarości dla dolnej części\n",
    "\n",
    "    # Szukamy pionowych białych kolumn TYLKO w dolnej części obrazu\n",
    "    vertical_sum = np.sum(bottom_array > pixel_threshold, axis=0)  # Sumujemy piksele jaśniejsze niż próg w każdej kolumnie\n",
    "    white_columns = np.where(vertical_sum >= bottom_part.height * 1.0)[0]  # Kolumny, które mają powyżej 95% białych pikseli\n",
    "\n",
    "    # Tworzymy nową listę kolumn, gdzie białe kolumny zostaną zmniejszone do 10% szerokości\n",
    "    reduced_columns = []\n",
    "    i = 0\n",
    "    while i < bottom_part.width:\n",
    "        if i in white_columns:\n",
    "            # Zmniejszamy szerokość białych kolumn do 10%\n",
    "            col_start = i\n",
    "            col_end = i\n",
    "            while col_end < bottom_part.width and col_end in white_columns:\n",
    "                col_end += 1\n",
    "            # Zostawiamy tylko 10% szerokości\n",
    "            reduced_width = max(1, int(0.1 * (col_end - col_start)))\n",
    "            reduced_columns.append(bottom_part.crop((col_start, 0, col_start + reduced_width, bottom_part.height)))\n",
    "            i = col_end  # Przeskakujemy białe kolumny\n",
    "        else:\n",
    "            # Normalna kolumna\n",
    "            reduced_columns.append(bottom_part.crop((i, 0, i + 1, bottom_part.height)))\n",
    "            i += 1\n",
    "\n",
    "    # Łączymy wszystkie kolumny w nowy obraz dla dolnej części\n",
    "    new_bottom_width = sum([col.width for col in reduced_columns])\n",
    "    reduced_bottom_part = Image.new('RGB', (new_bottom_width, bottom_part.height))\n",
    "    \n",
    "    x_offset = 0\n",
    "    for col in reduced_columns:\n",
    "        reduced_bottom_part.paste(col, (x_offset, 0))\n",
    "        x_offset += col.width\n",
    "\n",
    "    # Dopasowanie szerokości górnej części do szerokości dolnej części\n",
    "    top_part_width = top_part.width\n",
    "    width_diff = top_part_width - new_bottom_width\n",
    "\n",
    "    if width_diff > 0:\n",
    "        # Przycinamy górną część po bokach, aby dopasować do dolnej części\n",
    "        left_crop = width_diff // 2\n",
    "        right_crop = width_diff - left_crop\n",
    "        new_top_part = top_part.crop((left_crop, 0, top_part_width - right_crop, top_part.height))\n",
    "    else:\n",
    "        # Jeśli szerokość górnej części jest mniejsza lub równa, nie przycinamy\n",
    "        new_top_part = top_part\n",
    "\n",
    "    # Łączenie górnej i dolnej części\n",
    "    default_color = tuple(np.array(img_color)[0,0])\n",
    "    final_img = Image.new('RGB', (new_bottom_width, new_top_part.height + new_gap+ reduced_bottom_part.height), color=default_color)\n",
    "    final_img.paste(new_top_part, (40, 0)) # colorbar odstaje 48px\n",
    "    final_img.paste(reduced_bottom_part, (0, new_top_part.height + new_gap))\n",
    "\n",
    "    # Wyświetlanie i zapisywanie obrazu\n",
    "    final_img.show()\n",
    "    final_img.save(output_path)\n",
    "    print(f\"Wynikowy obraz zapisany do {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eksperymenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experiment():\n",
    "    def __init__(self, category, experiment_name, benchmark_path, folders):\n",
    "        self.category = category #\n",
    "        self.experiment_name = experiment_name #\n",
    "\n",
    "        self.folders = folders #\n",
    "        self.filenames = [] #\n",
    "        self.benchmark_path = benchmark_path #\n",
    "        self.mu_a = [] #\n",
    "        self.all_cubes_names = [] #\n",
    "        self.params_types = [] #\n",
    "\n",
    "        self.cubes_path = 'CUBES'\n",
    "\n",
    "        self.file_paths = []\n",
    "\n",
    "\n",
    "    def get_all_cubes_names_from_filenames(self):\n",
    "        self.all_cubes_names = []\n",
    "        for f_list in self.filenames:\n",
    "            this_dir_cube_names = []\n",
    "            for fn in f_list:\n",
    "                cube_name = fn[:-5]\n",
    "                this_dir_cube_names.append(cube_name)\n",
    "            self.all_cubes_names.append(this_dir_cube_names)\n",
    "\n",
    "\n",
    "    def file_paths_from_filenames(self):\n",
    "        self.file_paths = []\n",
    "        for dir_id in range(len(self.folders)):\n",
    "            this_folder_paths = []\n",
    "            for fn_id in range(len(self.filenames[dir_id])):\n",
    "                path = '/'.join([self.cubes_path, self.folders[dir_id], self.filenames[dir_id][fn_id]])\n",
    "                this_folder_paths.append(path)\n",
    "            self.file_paths.append(this_folder_paths)\n",
    "\n",
    "\n",
    "    def get_filenames_from_folders(self):\n",
    "        self.filenames = []\n",
    "        for f in self.folders:\n",
    "            path = '/'.join([self.cubes_path, f])\n",
    "            file_list = os.listdir(path)\n",
    "            self.filenames.append(file_list)\n",
    "        self.file_paths_from_filenames()\n",
    "\n",
    "\n",
    "    def mua_per_dir(self, mua_list):\n",
    "        self.mu_a = []\n",
    "        for mua, fn in zip(mua_list, self.filenames):\n",
    "            self.mu_a.append([mua] * len(fn)) # łączenie tablic\n",
    "\n",
    "\n",
    "    def params_types_per_dir(self, params_types):\n",
    "        self.params_types = []\n",
    "        for pt, fn in zip(params_types, self.filenames):\n",
    "            self.params_types.append([pt] * len(fn))\n",
    "        print(self.params_types)\n",
    "\n",
    "\n",
    "    def mua_from_file(self):\n",
    "        self.mu_a = []\n",
    "        for dir_id in range(len(self.folders)):\n",
    "            this_folder_mua = []\n",
    "            for p in self.cubes_path:\n",
    "                with open(p, 'r') as f:\n",
    "                    cub = f.read()\n",
    "                mua = cub['mu_a']\n",
    "                this_folder_mua.append(mua)\n",
    "            self.mu_a.append(this_folder_mua)\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        # load from files -> all_cubes\n",
    "        all_cubes = []\n",
    "        dir_num = len(self.folders)\n",
    "        for dir_id in range(dir_num):\n",
    "            print(self.folders[dir_id])\n",
    "            path_list = self.file_paths[dir_id]\n",
    "            all_cubes += load_from_filename_list(path_list)\n",
    "        # przydatne cubes - nazwy\n",
    "        benchamrk_cube_name = self.benchmark_path[-1][:-5]\n",
    "        first_cubes_names = [fn[0] for fn in self.all_cubes_names]\n",
    "        mid_cubes_names = [fn[len(fn)//2] for fn in self.all_cubes_names]\n",
    "        last_cubes_names = [fn[-1] for fn in self.all_cubes_names]\n",
    "        # przydatne cubes - instancje\n",
    "        first_cubes = [cube_by_name(all_cubes, fn) for fn in first_cubes_names]\n",
    "        mid_cubes = [cube_by_name(all_cubes, fn) for fn in mid_cubes_names]\n",
    "        last_cubes = [cube_by_name(all_cubes, fn) for fn in last_cubes_names]\n",
    "        benchmark_cube = cube_by_name(all_cubes, benchamrk_cube_name)\n",
    "        \n",
    "        print('================================')\n",
    "        print('wczytano z plików')\n",
    "        print('len(all_cubes)', len(all_cubes))\n",
    "\n",
    "        # przypisz właściwości podstawowe\n",
    "        all_cubes_names = sum(self.all_cubes_names, []) # łączenie tablic\n",
    "        all_mu_a = sum(self.mu_a, [])\n",
    "        all_filename_list = sum(self.file_paths, [])\n",
    "        all_dir_list = self.folders.copy()\n",
    "        params_types = sum(self.params_types, [])\n",
    "        print('params_types', params_types)\n",
    "        for cub, name, mu_a, fn, fndir, param_t in zip(all_cubes, all_cubes_names, all_mu_a, all_filename_list, all_dir_list, params_types):\n",
    "            cub['name'] = name\n",
    "            cub['photon_weight'] = 1.0\n",
    "            cub['normalized_already'] = False\n",
    "            cub['mu_a'] = mu_a\n",
    "            cub['file_path'] = fn\n",
    "            cub['file_dir_path'] = fndir\n",
    "            cub['params_type'] = param_t\n",
    "\n",
    "        # jeśli random cube było wcześniej - wyrzucić je\n",
    "        if all_cubes[0]['name'] == 'random_cube':\n",
    "            all_cubes = all_cubes[1:]\n",
    "\n",
    "        # dodaj losową próbę kontrolną\n",
    "        add_random_cube(all_cubes)\n",
    "        print('dodano kontrolną próbę losową')\n",
    "\n",
    "        # print info\n",
    "        print('================================')\n",
    "        print(\"shape\", all_cubes[0]['cube'].shape)\n",
    "        print(\"size\", all_cubes[0]['cube'].size)\n",
    "        print(\"len\", len(all_cubes))\n",
    "\n",
    "        for c in all_cubes:\n",
    "            normalization(c)\n",
    "        print('zakończono normalizację')\n",
    "\n",
    "        print('================================')\n",
    "        small_stat_table(all_cubes)\n",
    "        print('wydrukowano małą tabelkę ze statystykami')\n",
    "\n",
    "        # USTAWIANIE WARTOŚCI SKRAJNYCH DO TRANSFORMACJI\n",
    "        max_my = 2.5\n",
    "        max_org = 1.1\n",
    "        brpoint_my = 0.25\n",
    "        brpoint_org = 0.001\n",
    "\n",
    "        quantile_transformer = None\n",
    "        power_transformer = None\n",
    "        global_training_arr = None\n",
    "\n",
    "        org_quantile_transformer = None\n",
    "        org_power_transformer = None\n",
    "        org_global_training_arr = None\n",
    "        my_quantile_transformer = None\n",
    "        my_power_transformer = None\n",
    "        my_global_training_arr = None\n",
    "\n",
    "        print('================================')\n",
    "        setup_transformers(all_cubes)\n",
    "        print(my_quantile_transformer)\n",
    "        print('ustawiono modele do transformacji w rozkład normalny')\n",
    "\n",
    "        print('================================')\n",
    "        csv_out = 'out_csv/' + 'print_stats_' + self.category + '_' + self.experiment_name + '.csv'\n",
    "        print_stats(all_cubes, csv_out=csv_out)\n",
    "        print('wydrukowano dużą tabelkę ze statystykami')\n",
    "\n",
    "        print('================================')\n",
    "        print('print_all_normal_charts')\n",
    "        for i in range(-1,5):\n",
    "            draw_fun2(ttest_preprocess, start=-10, stop=10, title=\"my params draw fun\", typ='my', dec=i)\n",
    "            draw_fun2(ttest_preprocess, start=-10, stop=10, title=\"org params draw fun\", typ='org', dec=i)\n",
    "            print_all_normal_charts([benchmark_cube], dec=i)\n",
    "            print_all_normal_charts(first_cubes, dec=i)\n",
    "            print_all_normal_charts(mid_cubes, dec=i)\n",
    "            print_all_normal_charts(last_cubes, dec=i)\n",
    "        print('print_all_normal_charts - gotowe')\n",
    "\n",
    "        print('================================')\n",
    "        print('porównanie w tabeli')\n",
    "        csv_out = 'out_csv/' + 'compare_all_cubes_universal_' + self.category + '_' + self.experiment_name + '.csv'\n",
    "        compare_all_cubes_universal(all_cubes, benchmark_cube, csv_out=csv_out)\n",
    "        print('porównanie w tabeli - koniec')\n",
    "\n",
    "        print('================================')\n",
    "        print('tabela korelacji ')\n",
    "        corr_table_universal(all_cubes)\n",
    "\n",
    "        print('================================')\n",
    "        kruskal_wrapper(all_cubes)\n",
    "        print('================================')\n",
    "        anova_wrapper(all_cubes)\n",
    "\n",
    "        print('================================')\n",
    "        make_frames(all_cubes)\n",
    "        # przydatne cubes - instancje - aktualizacja\n",
    "        first_cubes = [cube_by_name(all_cubes, fn) for fn in first_cubes_names]\n",
    "        mid_cubes = [cube_by_name(all_cubes, fn) for fn in mid_cubes_names]\n",
    "        last_cubes = [cube_by_name(all_cubes, fn) for fn in last_cubes_names]\n",
    "        benchmark_cube = cube_by_name(all_cubes, benchamrk_cube_name)\n",
    "        print('utworzono ramki')\n",
    "\n",
    "        print('================================')\n",
    "        print(\"wykresy - porównanie w siatce\")\n",
    "        frame_id = 6 # średnie\n",
    "        bins_per_cm = all_cubes[0]['bins_per_1_cm']\n",
    "        title = all_cubes[0]['frame_names'][frame_id]\n",
    "        len_first_cubes = len(first_cubes)\n",
    "        len_mid_cubes = len(mid_cubes)\n",
    "        len_last_cubes = len(last_cubes)\n",
    "        cube_list = []\n",
    "        for i in range(4):\n",
    "            if len_first_cubes > i:\n",
    "                cube_list.append(first_cubes[i])\n",
    "                continue\n",
    "            j = i - len_first_cubes\n",
    "            if len_mid_cubes > j:\n",
    "                cube_list.append(mid_cubes[j])\n",
    "                continue\n",
    "            k = j - len_mid_cubes\n",
    "            if len_last_cubes > k:\n",
    "                cube_list.append(last_cubes[k])\n",
    "                continue\n",
    "            l = k - len_last_cubes\n",
    "            if 1 > l:\n",
    "                cube_list.append(benchmark_cube)\n",
    "                continue\n",
    "        arr_list = [cube['frames'][frame_id] for cube in cube_list]\n",
    "        # titles = [title for _ in range(len(arr_list))]\n",
    "        heatmap2d_grid_shared_colorbar(arr_list, bins_per_cm, grid_shape=(2,2), cube_list=cube_list, main_title=title, norm=\"log\")\n",
    "\n",
    "        print('================================')\n",
    "        print(\"przytnij zdjęcie w siatce\")\n",
    "        img_out = 'out_csv/' + 'chart4_img' + self.category + '_' + self.experiment_name + '.png'\n",
    "        detect_and_split_image_with_vertical_trim('chart4_img/chart4.png', img_out, gap_threshold=16, pixel_threshold=220, new_gap=20)\n",
    "\n",
    "        print('================================')\n",
    "        print('benchmark heatmap')\n",
    "        cub = benchmark_cube\n",
    "        i = 6\n",
    "        bins_per_cm = cub['bins_per_1_cm']\n",
    "        title = cub['frame_shortnames'][i]\n",
    "        arr = cub['frames'][i]\n",
    "        cube = all_cubes[c]\n",
    "        heatmap2d(arr, bins_per_cm, cube=cube, title=title)\n",
    "        # plot2_heatmap2d(arr1, arr2, title1=None, title2=None)\n",
    "\n",
    "        print('================================')\n",
    "        print('wszystkie przekroje')\n",
    "        [print(cube['name']) for cube in all_cubes]\n",
    "\n",
    "        for i in range(len(all_cubes)):\n",
    "            print(all_cubes[i]['name'])\n",
    "            print_all_frames(all_cubes[i:i+1])\n",
    "\n",
    "        idx = 6 # średnia\n",
    "        benchmark_arr = benchmark_cube['frames'][idx]\n",
    "        other_frame_arrs = [c['frames'][idx] for c in all_cubes]\n",
    "\n",
    "        print('================================')\n",
    "        print('print_all_normal_charts - 2D przekroje średnie')\n",
    "        for i in range(-1,5):\n",
    "            print_all_normal_charts([benchmark_cube], all_arrs=[benchmark_arr], dec=i)\n",
    "        print('print_all_normal_charts - gotowe - 2D przekroje średnie')\n",
    "\n",
    "        print('================================')\n",
    "        print('porównanie średnich - przekroje 2D średnie')\n",
    "        csv_out = 'out_csv/' + 'print_stats_' + 'srednie_2D_' + self.category + '_' + self.experiment_name + '.csv'\n",
    "        print('statystyki')\n",
    "        print()\n",
    "        print_stats(all_cubes, other_frame_arrs, csv_out=csv_out)\n",
    "        print()\n",
    "        print('porównanie')\n",
    "        print()\n",
    "        csv_out = 'out_csv/' + 'compare_all_cubes_universal_' + 'srednie_2D_' + self.category + '_' + self.experiment_name + '.csv'\n",
    "        compare_all_cubes_universal(all_cubes, benchmark_cube, other_arr_list=other_frame_arrs, benchmark_arr=benchmark_arr, csv_out=csv_out)\n",
    "        print()\n",
    "        print('tabela korelacji (parametryczne) - przekroje 2D średnie')\n",
    "        print()\n",
    "        corr_table_universal(all_cubes, other_frame_arrs)\n",
    "\n",
    "        idx = 1 # środkowy przekrój\n",
    "        benchmark_arr = benchmark_cube['frames'][idx]\n",
    "        other_frame_arrs = [c['frames'][idx] for c in all_cubes]\n",
    "\n",
    "        print('================================')\n",
    "        print('print_all_normal_charts - 2D środkowe przekroje')\n",
    "        for i in range(-1,5):\n",
    "            print_all_normal_charts([benchmark_cube], all_arrs=[benchmark_arr], dec=i)\n",
    "        print('print_all_normal_charts - gotowe - 2D środkowe przekroje')\n",
    "\n",
    "        print('================================')\n",
    "        print('porównanie średnich - środkowe przekroje 2D')\n",
    "        csv_out = 'out_csv/' + 'print_stats_' + 'srodkowe_przekroje_2D_' + self.category + '_' + self.experiment_name + '.csv'\n",
    "        print('statystyki')\n",
    "        print()\n",
    "        print_stats(all_cubes, other_frame_arrs, csv_out=csv_out)\n",
    "        print()\n",
    "        print('porównanie')\n",
    "        print()\n",
    "        csv_out = 'out_csv/' + 'compare_all_cubes_universal_' + 'srodkowe_przekroje_2D_' + self.category + '_' + self.experiment_name + '.csv'\n",
    "        compare_all_cubes_universal(all_cubes, benchmark_cube, other_arr_list=other_frame_arrs, benchmark_arr=benchmark_arr, csv_out=csv_out)\n",
    "        print()\n",
    "        print('tabela korelacji (parametryczne) - środkowe przekroje 2D')\n",
    "        print()\n",
    "        corr_table_universal(all_cubes, other_frame_arrs)\n",
    "\n",
    "        idx = 16\n",
    "        samples = all_cubes[0]['frames'][idx]\n",
    "        x_tics = [ val / bins_per_cm for val in range(len(samples)-1, -1, -1) ]\n",
    "        print(x_tics)\n",
    "        fig, ax = plt.subplots()\n",
    "        for c in all_cubes:\n",
    "            samples = c['frames'][idx]\n",
    "            ax.plot(x_tics, samples, label=c['name'])\n",
    "            ax.legend()\n",
    "        plt.title(\"Średnia zmiana współczynnika fluencji względnej Fl wzdłuż osi z\")\n",
    "        plt.xlabel(\"z [cm]\")\n",
    "        plt.ylabel(r\"Fl [$1/cm^2$]\")\n",
    "        plt.yscale('log')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        print(len(samples))\n",
    "\n",
    "        idx = 16\n",
    "        samples = all_cubes[0]['frames'][idx]\n",
    "        x_tics = [ val / bins_per_cm for val in range(len(samples)-1, -1, -1) ]\n",
    "        print(x_tics)\n",
    "        fig, ax = plt.subplots()\n",
    "        for c in all_cubes:\n",
    "            samples = c['frames'][idx]\n",
    "            ax.plot(x_tics, samples, label=c['name'])\n",
    "            # ax.legend()\n",
    "        plt.title(\"Średnia zmiana współczynnika fluencji względnej Fl wzdłuż osi z\")\n",
    "        plt.xlabel(\"z [cm]\")\n",
    "        plt.ylabel(r\"Fl [$1/cm^2$]\")\n",
    "        plt.yscale('log')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        print(len(samples))\n",
    "\n",
    "        idx = 17\n",
    "        samples = all_cubes[0]['frames'][idx]\n",
    "        x_tics = [ val / bins_per_cm for val in range(len(samples)-1, -1, -1) ]\n",
    "        print(x_tics)\n",
    "        fig, ax = plt.subplots()\n",
    "        for c in all_cubes:\n",
    "            samples = c['frames'][idx]\n",
    "            ax.plot(x_tics, samples, label=c['name'])\n",
    "            ax.legend()\n",
    "        plt.title(\"Średnia zmiana współczynnika fluencji względnej Fl wzdłuż osi y\")\n",
    "        plt.xlabel(\"y [cm]\")\n",
    "        plt.ylabel(r\"Fl [$1/cm^2$]\")\n",
    "        plt.yscale('log')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        print(len(samples))\n",
    "\n",
    "        idx = 17\n",
    "        samples = all_cubes[0]['frames'][idx]\n",
    "        x_tics = [ val / bins_per_cm for val in range(len(samples)-1, -1, -1) ]\n",
    "        print(x_tics)\n",
    "        fig, ax = plt.subplots()\n",
    "        for c in all_cubes:\n",
    "            samples = c['frames'][idx]\n",
    "            ax.plot(x_tics, samples, label=c['name'])\n",
    "            # ax.legend()\n",
    "        plt.title(\"Średnia zmiana współczynnika fluencji względnej Fl wzdłuż osi y\")\n",
    "        plt.xlabel(\"y [cm]\")\n",
    "        plt.ylabel(r\"Fl [$1/cm^2$]\")\n",
    "        plt.yscale('log')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        print(len(samples))\n",
    "\n",
    "        idx = 18\n",
    "        samples = all_cubes[0]['frames'][idx]\n",
    "        x_tics = [ val / bins_per_cm for val in range(len(samples)-1, -1, -1) ]\n",
    "        print(x_tics)\n",
    "        fig, ax = plt.subplots()\n",
    "        for c in all_cubes:\n",
    "            samples = c['frames'][idx]\n",
    "            ax.plot(x_tics, samples, label=c['name'])\n",
    "            ax.legend()\n",
    "        plt.title(\"Średnia zmiana współczynnika fluencji względnej Fl wzdłuż osi x\")\n",
    "        plt.xlabel(\"x [cm]\")\n",
    "        plt.ylabel(r\"Fl [$1/cm^2$]\")\n",
    "        plt.yscale('log')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        print(len(samples))\n",
    "\n",
    "        idx = 18\n",
    "        samples = all_cubes[0]['frames'][idx]\n",
    "        x_tics = [ val / bins_per_cm for val in range(len(samples)-1, -1, -1) ]\n",
    "        print(x_tics)\n",
    "        fig, ax = plt.subplots()\n",
    "        for c in all_cubes:\n",
    "            samples = c['frames'][idx]\n",
    "            ax.plot(x_tics, samples, label=c['name'])\n",
    "            # ax.legend()\n",
    "        plt.title(\"Średnia zmiana współczynnika fluencji względnej Fl wzdłuż osi x\")\n",
    "        plt.xlabel(\"x [cm]\")\n",
    "        plt.ylabel(r\"Fl [$1/cm^2$]\")\n",
    "        plt.yscale('log')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        print(len(samples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# porównanie python - język c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mc456 my_params 10k-100mln\n",
      "mc456_p my-params 10k-10mln\n",
      "trimming\n",
      "shape (181, 181, 241)\n",
      "shape (180, 180, 240)\n",
      "trimming\n",
      "shape (181, 181, 241)\n",
      "shape (180, 180, 240)\n",
      "trimming\n",
      "shape (181, 181, 241)\n",
      "shape (180, 180, 240)\n",
      "trimming\n",
      "shape (181, 181, 241)\n",
      "shape (180, 180, 240)\n",
      "================================\n",
      "wczytano z plików\n",
      "len(all_cubes) 9\n",
      "params_types []\n",
      "7776000\n",
      "(180, 180, 240)\n",
      "100000000.00000015\n",
      "dodano kontrolną próbę losową\n",
      "================================\n",
      "shape (180, 180, 240)\n",
      "size 7776000\n",
      "len 10\n",
      "zakończono normalizację\n",
      "================================\n",
      "       max                          nameparams_type             max val           zero_perc\n",
      "       max                 0 random_cube    org_my                0.26                0.00\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'params_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m test\u001b[38;5;241m.\u001b[39mmua_per_dir([\u001b[38;5;241m0.37\u001b[39m, \u001b[38;5;241m0.37\u001b[39m])\n\u001b[0;32m      9\u001b[0m test\u001b[38;5;241m.\u001b[39mparams_type_per_dir([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 10\u001b[0m test\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[1;32mIn[36], line 135\u001b[0m, in \u001b[0;36mexperiment.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzakończono normalizację\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m================================\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 135\u001b[0m small_stat_table(all_cubes)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwydrukowano małą tabelkę ze statystykami\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# USTAWIANIE WARTOŚCI SKRAJNYCH DO TRANSFORMACJI\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 559\u001b[0m, in \u001b[0;36msmall_stat_table\u001b[1;34m(all_cubes)\u001b[0m\n\u001b[0;32m    557\u001b[0m maxi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(arr)\n\u001b[0;32m    558\u001b[0m zero_perc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(arr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m arr\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 559\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmaxi\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>20,.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mzero_perc\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>20,.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    560\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'params_type'"
     ]
    }
   ],
   "source": [
    "test = experiment(category = 'porównanie python - język c',\n",
    "                  experiment_name = '',\n",
    "                  benchmark_path = ['CUBES', 'mc456 my_params 10k-100mln', 'mc456_mc_100mln_my_params_cube.json'],\n",
    "                  folders = ['mc456 my_params 10k-100mln', 'mc456_p my-params 10k-10mln']\n",
    "                  )\n",
    "test.get_filenames_from_folders()\n",
    "test.get_all_cubes_names_from_filenames()\n",
    "test.mua_per_dir([0.37, 0.37])\n",
    "test.params_types_per_dir(['my', 'my'])\n",
    "test.run()\n",
    "\n",
    "# mc456 my_params 10k-100mln\n",
    "# mc456_p my-params 10k-10mln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PORÓWNYWANIE OGÓLNE - my params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = experiment(category = 'PORÓWNYWANIE OGÓLNE - my params',\n",
    "                  experiment_name = '',\n",
    "                  benchmark_path = ['CUBES', 'mc456 my_params 10k-100mln', 'mc456_mc_100mln_my_params_cube.json'],\n",
    "                  folders = ['mati-sim my_params 10k-100mln', 'mc456 my_params 10k-100mln']\n",
    "                  )\n",
    "test.get_filenames_from_folders()\n",
    "test.get_all_cubes_names_from_filenames()\n",
    "test.mua_per_dir([0.37, 0.37])\n",
    "test.params_types_per_dir(['my', 'my'])\n",
    "test.run()\n",
    "\n",
    "# mati-sim my_params 10k-100mln\n",
    "# mc456 my_params 10k-100mln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PORÓWNYWANIE OGÓLNE - org params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = experiment(category = 'PORÓWNYWANIE OGÓLNE - org params',\n",
    "                  experiment_name = '',\n",
    "                  benchmark_path = ['CUBES', 'mc456 org_params 10k-100mln', 'mc456_mc_100mln_org_params_cube.json'],\n",
    "                  folders = ['mati-sim org_params 10k-10mln', 'mc456 org_params 10k-100mln']\n",
    "                  )\n",
    "test.get_filenames_from_folders()\n",
    "test.get_all_cubes_names_from_filenames()\n",
    "test.mua_per_dir([1.673, 1.673])\n",
    "test.params_types_per_dir(['org', 'org'])\n",
    "test.run()\n",
    "\n",
    "# mati-sim org_params 10k-10mln\n",
    "# mc456 org_params 10k-100mln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specjalistyczne mc456 rozne g (0.0-1.0 z krokiem 0.1) 100mln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = experiment(category = 'specjalistyczne mc456 rozne g (0.0-1.0 z krokiem 0.1) 100mln',\n",
    "                  experiment_name = '',\n",
    "                  benchmark_path = ['CUBES', 'mc456 rozne g (0.0-1.0 z krokiem 0.1) 100mln', 'mc456_mc_100mln_my_params_tiss_id_4_g_0_9__cube.json'],\n",
    "                  folders = ['mc456 rozne g (0.0-1.0 z krokiem 0.1) 100mln']\n",
    "                  )\n",
    "test.get_filenames_from_folders()\n",
    "test.get_all_cubes_names_from_filenames()\n",
    "test.mua_per_dir([0.37])\n",
    "test.params_types_per_dir(['my'])\n",
    "test.run()\n",
    "\n",
    "# mc456 rozne g (0.0-1.0 z krokiem 0.1) 100mln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specjalistyczne mati-sim my_params 10k g {0, 0.5, 0.9, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = experiment(category = 'specjalistyczne mati-sim my_params 10k g {0, 0.5, 0.9, 1}',\n",
    "                  experiment_name = '',\n",
    "                  benchmark_path = ['CUBES', 'mati-sim my_params 10k g {0, 0.5, 0.9, 1}', 'mati-sim my_params 10k g_0_9_cube.json'],\n",
    "                  folders = ['mati-sim my_params 10k g {0, 0.5, 0.9, 1}'],\n",
    "                  )\n",
    "test.get_filenames_from_folders()\n",
    "test.get_all_cubes_names_from_filenames()\n",
    "test.mua_per_dir([0.37])\n",
    "test.params_types_per_dir(['my'])\n",
    "test.run()\n",
    "\n",
    "\n",
    "# mati-sim my_params 10k g {0, 0.5, 0.9, 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specjalistyczne mc456 rozne_skóry_z_tabeli (8 rodzajów) 100mln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = experiment(category = 'specjalistyczne mc456 rozne_skóry_z_tabeli (8 rodzajów) 100mln',\n",
    "                  experiment_name = '',\n",
    "                  benchmark_path = ['CUBES', 'mc456 rozne_skóry_z_tabeli (8 rodzajów) 100mln', 'mc456_mc_100mln_my_params_tiss_id_4_cube.json'],\n",
    "                  folders = ['mc456 rozne_skóry_z_tabeli (8 rodzajów) 100mln'],\n",
    "                  )\n",
    "test.get_filenames_from_folders()\n",
    "test.get_all_cubes_names_from_filenames()\n",
    "test.mua_from_file()\n",
    "test.params_types_per_dir(['my'])\n",
    "test.run()\n",
    "\n",
    "# mc456 rozne_skóry_z_tabeli (8 rodzajów) 100mln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specjalistyczne mc456 my-params light-sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = experiment(category = 'specjalistyczne mc456 my-params light-sources',\n",
    "                  experiment_name = '',\n",
    "                  benchmark_path = ['CUBES', 'mc456 my-params light-sources', 'mc456_mc_100mln_my_params_tiss_id_4_g_0_9_ls_down_cube.json'],\n",
    "                  folders = ['mc456 my-params light-sources'],\n",
    "                  )\n",
    "test.get_filenames_from_folders()\n",
    "test.get_all_cubes_names_from_filenames()\n",
    "test.mua_per_dir([0.37])\n",
    "test.params_types_per_dir(['my'])\n",
    "test.run()\n",
    "\n",
    "# mc456 my-params light-sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dodatkowe mati-sim 2-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = experiment(category = 'dodatkowe mati-sim 2-layers',\n",
    "                  experiment_name = '',\n",
    "                  benchmark_path = ['CUBES', 'mati-sim 2-layers', 'mati-sim 2-layers 1_000_000_photons_cube.json'],\n",
    "                  folders = ['mati-sim 2-layers'],\n",
    "                  )\n",
    "test.get_filenames_from_folders()\n",
    "test.get_all_cubes_names_from_filenames()\n",
    "test.mua_per_dir([0.37])\n",
    "test.params_types_per_dir(['my'])\n",
    "test.run()\n",
    "\n",
    "# mati-sim 2-layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dodatkowe mati-sim 3-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = experiment(category = 'dodatkowe mati-sim 3-layers',\n",
    "                  experiment_name = '',\n",
    "                  benchmark_path = ['CUBES', 'mati-sim 3-layers', 'mati-sim 3-layers 1_000_000_cube.json'],\n",
    "                  folders = ['mati-sim 3-layers'],\n",
    "                  )\n",
    "test.get_filenames_from_folders()\n",
    "test.get_all_cubes_names_from_filenames()\n",
    "test.mua_per_dir([0.37])\n",
    "test.params_types_per_dir(['my'])\n",
    "test.run()\n",
    "\n",
    "# mati-sim 3-layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dodatkowe mati-sim veins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = experiment(category = 'dodatkowe mati-sim veins',\n",
    "                  experiment_name = '',\n",
    "                  benchmark_path = ['CUBES', 'mati-sim veins', 'mati-sim veins 100_000_fotonów_cube.json'],\n",
    "                  folders = ['mati-sim veins'],\n",
    "                  )\n",
    "test.get_filenames_from_folders()\n",
    "test.get_all_cubes_names_from_filenames()\n",
    "test.mua_per_dir([0.37])\n",
    "test.params_types_per_dir(['my'])\n",
    "test.run()\n",
    "\n",
    "# mati-sim veins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mati-sim 2-layers',\n",
       " 'mati-sim 3-layers',\n",
       " 'mati-sim my_params 10k g {0, 0.5, 0.9, 1}',\n",
       " 'mati-sim my_params 10k-100mln',\n",
       " 'mati-sim org_params 10k-10mln',\n",
       " 'mati-sim veins',\n",
       " 'mc456 my-params light-sources',\n",
       " 'mc456 my_params 10k-100mln',\n",
       " 'mc456 org_params 10k-100mln',\n",
       " 'mc456 rozne g (0.0-1.0 z krokiem 0.1) 100mln',\n",
       " 'mc456 rozne_skóry_z_tabeli (8 rodzajów) 100mln',\n",
       " 'mc456_p my-params 10k-10mln']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('CUBES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123, 123, 123, 123]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[123]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porównanie python - język c:\n",
    "\n",
    "# mc456 my_params 10k-100mln\n",
    "# mc456_p my-params 10k-10mln\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PORÓWNYWANIE OGÓLNE:\n",
    "\n",
    "# mati-sim my_params 10k-100mln\n",
    "# mc456 my_params 10k-100mln\n",
    "\n",
    "# mati-sim org_params 10k-10mln\n",
    "# mc456 org_params 10k-100mln\n",
    "\n",
    "\n",
    "\n",
    "# specjalistyczne:\n",
    "\n",
    "# mc456 rozne g (0.0-1.0 z krokiem 0.1) 100mln\n",
    "\n",
    "# mati-sim my_params 10k g {0, 0.5, 0.9, 1}\n",
    "\n",
    "# mc456 rozne_skóry_z_tabeli (8 rodzajów) 100mln\n",
    "\n",
    "# mc456 my-params light-sources\n",
    "\n",
    "\n",
    "\n",
    "# dodatkowe:\n",
    "\n",
    "# mati-sim 2-layers\n",
    "\n",
    "# mati-sim 3-layers\n",
    "\n",
    "# mati-sim veins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
